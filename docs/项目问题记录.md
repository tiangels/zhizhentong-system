# 项目实施过程中产生的问题列表
## 1.项目目录结构要清晰。
整个项目的目录结构应该清晰，在项目构建时应考虑。
## 2.数据预处理相关问题。
### 2.0 数据清洗
在数据清洗过程中，对于医学上的特殊字符（单位符号【mg ⬆️ 】、检验符号【+代表阳性】等等），还是要酌情考虑。
要对清洗后的数据质量进行
### 2.1 针对不同数据集需要有不同的处理机制。如：中文医疗对话数据集、VQA 数据集等。
#### 2.1.1 文本数据去重处理：
1. 数据量较大，确保内存不会溢出问题。
单机：分块加载数据并处理，记录数据的唯一标识（哈希值、语义指纹），通过唯一标识去重。
分布式：大数据架构 Sqprk/Flink，去重理念和单机一样。
2. 去重方案的选择问题：精准去重（完全相似【数据哈希】）、语义去重（判断向量之间的相似度【语义指纹】）、唯一 ID 去重（判断 ID 是否重复）
唯一 ID 适合：数据库中存储的结构化数据。
精准去重更适合：尽量保留数据，提升模型的性能，尤其是数据量少的情况。
语义去重：适合数据量多的场景，如：文本数据。
1. 新增数据去重问题。
   需借助数据库，通过数据持久化进行新增数据的去重。
- 单机：LevelDB
- 集群：Redis
- 向量化数据库：Milvus/FAISS
- 大规模数据集：HBase

#### 2.1.2 图文数据去重处理：
1.关于数据量大小，和文本去重一样，需要考虑内存问题
2.图文去重方案：
图文重复存在的情况：
- 图像和文本完全相同：直接删除
- 图像重复，文本不重复：根据相似度阈值去重，配合人工
- 文本重复，图像不重复：根据相似度阈值去重，配合人工
- 图像文本高度相似：根据相似度阈值去重，配合人工

## 3.分词相关问题。（中文、英文、垂直领域）
### 3.1 对于分词如何进行选择？如何评估分词的效果？
本系统使用 jieba 分词。
1.jieba 分词比较轻量，适合小数据集。还应考虑垂直领域的自定义分词，如医疗领域：“心肌梗死”不能分为“心肌”和“梗死”。
2.分词效果的评估
- 使用公开的分词数据集进行评估。如：医疗分词标注集（公开）
- 人工抽取一部分数据进行评估。

## 4.向量化相关问题。
### 4.1 图片向量化
图文数据集向量化：图片和文本应进行关联，且向量化的相似度应相似，处于同一特征空间，对于 rag 系统检索时，（文搜图、图搜文，才能正常匹配）。所以需要实现跨模态的向量存储。
