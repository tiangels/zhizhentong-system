"""
æ£€ç´¢æœåŠ¡æ¨¡å—
è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œä¸ºRAGç”Ÿæˆæä¾›ä¸Šä¸‹æ–‡
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
# import faiss  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä½¿ç”¨numpyæ›¿ä»£
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RetrievalService:
    """æ£€ç´¢æœåŠ¡ç±»ï¼Œè´Ÿè´£æ–‡æ¡£æ£€ç´¢å’Œæ’åº"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ£€ç´¢æœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.vector_db = None
        self.metadata_db = {}  # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.vector_dim = config.get('vector_dim', 384)
        self.max_results = config.get('max_results', 20)
        self.similarity_threshold = config.get('similarity_threshold', 0.8)
        
        # æ£€ç´¢ç­–ç•¥é…ç½®
        self.retrieval_strategies = {
            'semantic': self._semantic_retrieval,
            'hybrid': self._hybrid_retrieval,
            'rerank': self._rerank_retrieval
        }
        
        self.current_strategy = config.get('retrieval_strategy', 'semantic')
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_vector_db()
    
    def _init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            from langchain_chroma import Chroma
            from langchain_community.embeddings import HuggingFaceEmbeddings
            
            # åˆ›å»ºåµŒå…¥å‡½æ•°
            self.embedding_function = HuggingFaceEmbeddings(
                model_name=self.config.get('model_name', 'sentence-transformers/all-MiniLM-L6-v2'),
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # åˆ›å»ºChromaDBå‘é‡æ•°æ®åº“
            db_path = self.config.get('vector_db_path', './vector_db')
            collection_name = self.config.get('collection_name', 'medical_multimodal_vectors')
            self.vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=self.embedding_function,
                collection_name=collection_name
            )
            
            # ä¿ç•™numpyæ•°ç»„ä½œä¸ºå¤‡ç”¨
            self.vectors = []  # å­˜å‚¨å‘é‡
            self.documents = []  # å­˜å‚¨æ–‡æ¡£
            logger.info("Retrieval service initialized with ChromaDB")
        except Exception as e:
            logger.error(f"Error initializing retrieval service: {e}")
            raise
    
    def add_documents(self, vectors: np.ndarray, documents: List[Dict[str, Any]]):
        """
        æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            vectors: æ–‡æ¡£å‘é‡çŸ©é˜µ
            documents: æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨
        """
        try:
            if vectors.size == 0 or not documents:
                return
            
            # ç¡®ä¿å‘é‡æ˜¯äºŒç»´çš„
            if vectors.ndim == 1:
                vectors = vectors.reshape(1, -1)
            
            # å‡†å¤‡ChromaDBçš„æ–‡æ¡£å’Œå…ƒæ•°æ®
            chroma_documents = []
            chroma_metadatas = []
            
            for i, doc in enumerate(documents):
                # æå–æ–‡æ¡£å†…å®¹
                content = doc.get('content', doc.get('text', ''))
                if not content:
                    content = f"Document {i}: {doc.get('title', 'Untitled')}"
                
                chroma_documents.append(content)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'id': doc.get('id', f'doc_{i}'),
                    'title': doc.get('title', ''),
                    'source': doc.get('source', ''),
                    'category': doc.get('category', ''),
                    'timestamp': doc.get('timestamp', datetime.now().isoformat()),
                    'metadata': doc.get('metadata', {})
                }
                chroma_metadatas.append(metadata)
            
            # æ·»åŠ åˆ°ChromaDB
            self.vector_db.add_documents(
                documents=chroma_documents,
                metadatas=chroma_metadatas,
                embeddings=vectors.tolist()
            )
            
            # åŒæ—¶ä¿ç•™numpyæ•°ç»„ä½œä¸ºå¤‡ç”¨
            start_idx = len(self.vectors)
            self.vectors.extend(vectors.tolist())
            
            # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®åˆ°æœ¬åœ°å­—å…¸
            for i, doc in enumerate(documents):
                doc_id = start_idx + i
                self.metadata_db[doc_id] = {
                    'id': doc_id,
                    'content': doc.get('content', ''),
                    'title': doc.get('title', ''),
                    'source': doc.get('source', ''),
                    'category': doc.get('category', ''),
                    'timestamp': doc.get('timestamp', datetime.now().isoformat()),
                    'metadata': doc.get('metadata', {})
                }
            
            logger.info(f"Added {len(documents)} documents to ChromaDB and local storage")
            
        except Exception as e:
            logger.error(f"Error adding documents: {e}")
            raise
    
    def retrieve_documents(self, query_vector: np.ndarray, top_k: int = 10, 
                          strategy: str = None, query_text: str = None) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            strategy: æ£€ç´¢ç­–ç•¥
            
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            print("=" * 50)
            print("ğŸ” æ–‡æ¡£æ£€ç´¢å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹æ–‡æ¡£æ£€ç´¢...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šå‘é‡ç»´åº¦={len(query_vector) if query_vector is not None else 'None'}, top_k={top_k}, strategy='{strategy}'")
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹éªŒè¯æ£€ç´¢å‚æ•°")
            
            if self.vector_db._collection.count() == 0:
                logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢")
                return []
            
            # ä½¿ç”¨æŒ‡å®šçš„æ£€ç´¢ç­–ç•¥
            strategy = strategy or self.current_strategy
            if strategy not in self.retrieval_strategies:
                strategy = 'semantic'
                logger.info(f"ä½¿ç”¨é»˜è®¤æ£€ç´¢ç­–ç•¥: {strategy}")
            
            logger.info(f"æ£€ç´¢ç­–ç•¥: {strategy}")
            logger.info(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. æ‰§è¡Œæ£€ç´¢
            print("==========")
            print("æ‰§è¡Œæ£€ç´¢å¼€å§‹")
            print("==========")
            logger.info("æ‰§è¡Œæ£€ç´¢çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ‰§è¡Œæ–‡æ¡£æ£€ç´¢")
            
            # æ‰§è¡Œæ£€ç´¢
            results = self.retrieval_strategies[strategy](query_vector, top_k, query_text)
            logger.info(f"åˆæ­¥æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            filtered_results = [
                result for result in results 
                if result.get('similarity', 0) >= self.similarity_threshold
            ]
            logger.info(f"ç›¸ä¼¼åº¦è¿‡æ»¤å®Œæˆï¼Œä¿ç•™ {len(filtered_results)} ä¸ªæ–‡æ¡£ï¼ˆé˜ˆå€¼: {self.similarity_threshold:.1%}ï¼‰")
            
            # è®°å½•ç›¸ä¼¼åº¦åˆ†å¸ƒä¿¡æ¯
            if results:
                similarities = [result.get('similarity', 0) for result in results]
                max_sim = max(similarities)
                min_sim = min(similarities)
                avg_sim = sum(similarities) / len(similarities)
                logger.info(f"ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€é«˜={max_sim:.3f}, æœ€ä½={min_sim:.3f}, å¹³å‡={avg_sim:.3f}")
            
            final_results = filtered_results[:top_k]
            logger.info(f"æœ€ç»ˆè¿”å› {len(final_results)} ä¸ªæ–‡æ¡£")
            
            # å¦‚æœæ²¡æœ‰ç›¸å…³ç»“æœï¼Œè®°å½•è­¦å‘Š
            if not final_results:
                logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦å¤§äº {self.similarity_threshold:.1%} çš„ç›¸å…³æ–‡æ¡£")
                if results:
                    logger.warning(f"æœ€é«˜ç›¸ä¼¼åº¦ä»…ä¸º: {max_sim:.3f}")
                else:
                    logger.warning("çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            logger.info("æ‰§è¡Œæ£€ç´¢æˆåŠŸ")
            print("æ‰§è¡Œæ£€ç´¢ç»“æŸ")
            print("==========")
            
            # 4. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæ£€ç´¢ç»“æœ")
            logger.info(f"æ£€ç´¢ç»“æœæ•°é‡: {len(final_results)}")
            for i, result in enumerate(final_results):
                similarity = result.get('similarity', 0)
                title = result.get('title', 'æ— æ ‡é¢˜')
                logger.info(f"ç»“æœ {i+1}: ç›¸ä¼¼åº¦={similarity:.3f}, æ ‡é¢˜='{title}'")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ æ–‡æ¡£æ£€ç´¢å®Œæˆ")
            print("=" * 50)
            logger.info("æ–‡æ¡£æ£€ç´¢æˆåŠŸå®Œæˆ")
            
            return final_results
            
        except Exception as e:
            print("=" * 50)
            print("âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥")
            print("=" * 50)
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            logger.error(f"Error retrieving documents: {e}")
            return []
    
    def _semantic_retrieval(self, query_vector: np.ndarray, top_k: int, query_text: str = None) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æ£€ç´¢ç­–ç•¥
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            query_text: æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨äºChromaDBæ–‡æœ¬æœç´¢ï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ£€æŸ¥ChromaDBæ˜¯å¦æœ‰æ•°æ®
            if self.vector_db._collection.count() == 0:
                logger.warning("ChromaDBä¸­æ²¡æœ‰å¯æ£€ç´¢çš„æ–‡æ¡£")
                return []
            
            # ä½¿ç”¨ç”¨æˆ·çš„å®é™…æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œæ£€ç´¢
            if not query_text:
                logger.warning("æ²¡æœ‰æä¾›æŸ¥è¯¢æ–‡æœ¬ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æ£€ç´¢")
                return []
            
            logger.info(f"ä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œè¯­ä¹‰æ£€ç´¢: '{query_text}'")
            
            results = self.vector_db.similarity_search_with_score(
                query=query_text,
                k=top_k * 2,  # è·å–æ›´å¤šå€™é€‰ç»“æœç”¨äºåç»­è¿‡æ»¤
                filter=None
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼
            formatted_results = []
            for doc, score in results:
                result = {
                    'content': doc.page_content,
                    'title': doc.metadata.get('title', ''),
                    'source': doc.metadata.get('source', ''),
                    'category': doc.metadata.get('category', ''),
                    'timestamp': doc.metadata.get('timestamp', ''),
                    'metadata': doc.metadata,
                    'similarity': float(score)
                }
                formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error in semantic retrieval: {e}")
            return []
    
    def _hybrid_retrieval(self, query_vector: np.ndarray, top_k: int, query_text: str = None) -> List[Dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆè¯­ä¹‰ + å…³é”®è¯ï¼‰
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            query_text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # å…ˆè¿›è¡Œè¯­ä¹‰æ£€ç´¢
            semantic_results = self._semantic_retrieval(query_vector, top_k * 2, query_text)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…³é”®è¯æ£€ç´¢é€»è¾‘
            # ç›®å‰ç®€å•è¿”å›è¯­ä¹‰æ£€ç´¢ç»“æœ
            return semantic_results[:top_k]
            
        except Exception as e:
            logger.error(f"Error in hybrid retrieval: {e}")
            return []
    
    def _rerank_retrieval(self, query_vector: np.ndarray, top_k: int, query_text: str = None) -> List[Dict[str, Any]]:
        """
        é‡æ’åºæ£€ç´¢ç­–ç•¥
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            query_text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # å…ˆè·å–æ›´å¤šå€™é€‰æ–‡æ¡£
            candidates = self._semantic_retrieval(query_vector, top_k * 3, query_text)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ’åºé€»è¾‘
            # ç›®å‰ç®€å•è¿”å›å‰top_kä¸ªç»“æœ
            return candidates[:top_k]
            
        except Exception as e:
            logger.error(f"Error in rerank retrieval: {e}")
            return []
    
    def search_by_keywords(self, keywords: List[str], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        åŸºäºå…³é”®è¯æœç´¢æ–‡æ¡£
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            results = []
            keyword_set = set(keywords)
            
            for doc_id, doc in self.metadata_db.items():
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                content = doc.get('content', '').lower()
                title = doc.get('title', '').lower()
                
                # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
                matches = sum(1 for keyword in keyword_set if keyword.lower() in content or keyword.lower() in title)
                
                if matches > 0:
                    doc_copy = doc.copy()
                    doc_copy['keyword_matches'] = matches
                    doc_copy['similarity'] = matches / len(keyword_set)
                    results.append(doc_copy)
            
            # æŒ‰åŒ¹é…åº¦æ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Error in keyword search: {e}")
            return []
    
    def get_document_by_id(self, doc_id: int) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯
        """
        return self.metadata_db.get(doc_id)
    
    def get_documents_by_category(self, category: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ç±»åˆ«è·å–æ–‡æ¡£
        
        Args:
            category: æ–‡æ¡£ç±»åˆ«
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        try:
            results = []
            for doc in self.metadata_db.values():
                if doc.get('category', '').lower() == category.lower():
                    results.append(doc)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Error getting documents by category: {e}")
            return []
    
    def get_documents_by_source(self, source: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ¥æºè·å–æ–‡æ¡£
        
        Args:
            source: æ–‡æ¡£æ¥æº
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        try:
            results = []
            for doc in self.metadata_db.values():
                if doc.get('source', '').lower() == source.lower():
                    results.append(doc)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Error getting documents by source: {e}")
            return []
    
    def update_document(self, doc_id: int, updates: Dict[str, Any]):
        """
        æ›´æ–°æ–‡æ¡£ä¿¡æ¯
        
        Args:
            doc_id: æ–‡æ¡£ID
            updates: æ›´æ–°å†…å®¹
        """
        try:
            if doc_id in self.metadata_db:
                self.metadata_db[doc_id].update(updates)
                logger.info(f"Updated document {doc_id}")
            else:
                logger.warning(f"Document {doc_id} not found")
                
        except Exception as e:
            logger.error(f"Error updating document: {e}")
    
    def delete_document(self, doc_id: int):
        """
        åˆ é™¤æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
        """
        try:
            if doc_id in self.metadata_db:
                del self.metadata_db[doc_id]
                logger.info(f"Deleted document {doc_id}")
            else:
                logger.warning(f"Document {doc_id} not found")
                
        except Exception as e:
            logger.error(f"Error deleting document: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            total_docs = len(self.metadata_db)
            categories = set(doc.get('category', '') for doc in self.metadata_db.values())
            sources = set(doc.get('source', '') for doc in self.metadata_db.values())
            
            return {
                'total_documents': total_docs,
                'vector_database_size': self.vector_db._collection.count(),
                'categories': list(categories),
                'sources': list(sources),
                'retrieval_strategy': self.current_strategy,
                'similarity_threshold': self.similarity_threshold
            }
            
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {}
    
    def save_metadata(self, metadata_path: str):
        """
        ä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        try:
            os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.metadata_db, f, ensure_ascii=False, indent=2)
            logger.info(f"Metadata saved to {metadata_path}")
            
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
            raise
    
    def load_metadata(self, metadata_path: str):
        """
        ä»æ–‡ä»¶åŠ è½½å…ƒæ•°æ®
        
        Args:
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        try:
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata_db = json.load(f)
                logger.info(f"Metadata loaded from {metadata_path}")
            else:
                logger.warning(f"Metadata file not found: {metadata_path}")
                
        except Exception as e:
            logger.error(f"Error loading metadata: {e}")
            raise
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        try:
            self.vector_db.reset()
            self.metadata_db.clear()
            logger.info("All data cleared")
        except Exception as e:
            logger.error(f"Error clearing data: {e}")
            raise


class RetrievalServiceFactory:
    """æ£€ç´¢æœåŠ¡å·¥å‚ç±»"""
    
    @staticmethod
    def create_retrieval_service(config_path: str = None) -> RetrievalService:
        """
        åˆ›å»ºæ£€ç´¢æœåŠ¡å®ä¾‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€ç´¢æœåŠ¡å®ä¾‹
        """
        # é»˜è®¤é…ç½®
        default_config = {
            'vector_dim': 384,
            'max_results': 20,
            'similarity_threshold': 0.7,
            'retrieval_strategy': 'semantic'
        }
        
        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½é…ç½®
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"Error loading config file: {e}, using default config")
        
        return RetrievalService(default_config)


if __name__ == "__main__":
    # æµ‹è¯•æ£€ç´¢æœåŠ¡
    config = {
        'vector_dim': 384,
        'max_results': 10,
        'similarity_threshold': 0.5,
        'retrieval_strategy': 'semantic'
    }
    
    retrieval_service = RetrievalService(config)
    
    # æ¨¡æ‹Ÿæ·»åŠ æ–‡æ¡£
    test_documents = [
        {
            'content': 'èƒ¸ç—›æ˜¯å¿ƒè‚Œæ¢—æ­»çš„å¸¸è§ç—‡çŠ¶ï¼Œé€šå¸¸è¡¨ç°ä¸ºèƒ¸éª¨åå‹æ¦¨æ€§ç–¼ç—›',
            'title': 'å¿ƒè‚Œæ¢—æ­»ç—‡çŠ¶',
            'category': 'å¿ƒè¡€ç®¡ç–¾ç—…',
            'source': 'åŒ»å­¦æ•™ç§‘ä¹¦'
        },
        {
            'content': 'å‘¼å¸å›°éš¾å¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼ŒåŒ…æ‹¬è‚ºéƒ¨ç–¾ç—…å’Œå¿ƒè„ç–¾ç—…',
            'title': 'å‘¼å¸å›°éš¾åŸå› ',
            'category': 'å‘¼å¸ç³»ç»Ÿç–¾ç—…',
            'source': 'ä¸´åºŠæŒ‡å—'
        },
        {
            'content': 'å‘çƒ­æ˜¯æ„ŸæŸ“æ€§ç–¾ç—…çš„ä¸»è¦ç—‡çŠ¶ä¹‹ä¸€ï¼Œéœ€è¦åŠæ—¶æ²»ç–—',
            'title': 'å‘çƒ­ç—‡çŠ¶',
            'category': 'æ„ŸæŸ“æ€§ç–¾ç—…',
            'source': 'åŒ»å­¦æœŸåˆŠ'
        }
    ]
    
    # æ¨¡æ‹Ÿå‘é‡ï¼ˆå®é™…ä½¿ç”¨ä¸­åº”è¯¥é€šè¿‡å‘é‡åŒ–æœåŠ¡ç”Ÿæˆï¼‰
    test_vectors = np.random.rand(len(test_documents), 384).astype('float32')
    
    # æ·»åŠ æ–‡æ¡£
    retrieval_service.add_documents(test_vectors, test_documents)
    
    # æµ‹è¯•æ£€ç´¢
    query_vector = np.random.rand(384).astype('float32')
    results = retrieval_service.retrieve_documents(query_vector, top_k=2)
    
    print(f"Retrieved {len(results)} documents")
    for result in results:
        print(f"Title: {result['title']}, Similarity: {result['similarity']:.3f}")
    
    # æµ‹è¯•å…³é”®è¯æœç´¢
    keyword_results = retrieval_service.search_by_keywords(['èƒ¸ç—›', 'å¿ƒè‚Œæ¢—æ­»'], top_k=2)
    print(f"\nKeyword search results: {len(keyword_results)}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = retrieval_service.get_statistics()
    print(f"\nStatistics: {stats}")
