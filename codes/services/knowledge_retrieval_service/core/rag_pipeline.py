"""
RAGå®Œæ•´æµç¨‹æ¨¡å—
æ•´åˆå‘é‡åŒ–æœåŠ¡ã€æ£€ç´¢æœåŠ¡å’Œå¤§è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œå®ç°å®Œæ•´çš„RAGæµç¨‹
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
from datetime import datetime
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor

from .vector_service import VectorService, VectorServiceFactory
from .retrieval_service import RetrievalService, RetrievalServiceFactory
from .llm_service import LLMService, LLMServiceFactory
from .config_manager import ConfigManager

# å¯¼å…¥æ–‡æœ¬æ‘˜è¦æœåŠ¡
from .text_summarization_service import TextSummarizationService

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGPipeline:
    """RAGå®Œæ•´æµç¨‹ç±»ï¼Œæ•´åˆæ‰€æœ‰æœåŠ¡ç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–RAGæµç¨‹
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager()
        self.config = config or self.config_manager.config
        
        self.vector_service = None
        self.retrieval_service = None
        self.llm_service = None
        self.summarization_service = None
        
        # åˆå§‹åŒ–å„ä¸ªæœåŠ¡
        self._initialize_services()
        
        # çº¿ç¨‹æ± ç”¨äºå¹¶å‘å¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info("RAG Pipeline initialized successfully")
    
    def _initialize_services(self):
        """åˆå§‹åŒ–å„ä¸ªæœåŠ¡ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡
            vector_config = self.config_manager.get_vector_service_config()
            self.vector_service = VectorServiceFactory.create_vector_service()
            logger.info("Vector service initialized")
            
            # åˆå§‹åŒ–æ£€ç´¢æœåŠ¡
            retrieval_config = self.config_manager.get_retrieval_service_config()
            self.retrieval_service = RetrievalService(retrieval_config)
            logger.info("Retrieval service initialized")
            
            # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
            llm_config = self.config_manager.get_llm_service_config()
            config_path = self.config_manager.config_path
            self.llm_service = LLMServiceFactory.create_llm_service(config_path)
            logger.info("LLM service initialized")
            
            # åˆå§‹åŒ–æ–‡æœ¬æ‘˜è¦æœåŠ¡
            summarization_config = self.config.get('summarization_service', {})
            if summarization_config:
                model_path = summarization_config.get('model_path')
                self.summarization_service = TextSummarizationService(model_path)
                logger.info("Text summarization service initialized")
            else:
                logger.warning("No summarization service configuration found")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
    
    def add_documents(self, documents: List[Dict[str, Any]], 
                     vectorize_images: bool = False) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            vectorize_images: æ˜¯å¦å‘é‡åŒ–å›¾åƒ
            
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            print("=" * 50)
            print("ğŸ“„ RAGæ–‡æ¡£æ·»åŠ æµç¨‹å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹RAGæ–‡æ¡£æ·»åŠ æµç¨‹...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šæ–‡æ¡£æ•°é‡={len(documents)}, å‘é‡åŒ–å›¾åƒ={vectorize_images}")
            if not documents:
                logger.warning("No documents provided")
                return False
            
            for i, doc in enumerate(documents):
                doc_type = doc.get('type', 'text')
                doc_title = doc.get('title', 'æ— æ ‡é¢˜')
                logger.info(f"æ–‡æ¡£ {i+1}: ç±»å‹={doc_type}, æ ‡é¢˜='{doc_title}'")
            
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹åˆ†ç±»æ–‡æ¡£")
            
            # åˆ†ç¦»æ–‡æœ¬å’Œå›¾åƒæ–‡æ¡£
            text_docs = []
            image_docs = []
            
            for doc in documents:
                if doc.get('type') == 'image' and vectorize_images:
                    image_docs.append(doc)
                else:
                    text_docs.append(doc)
            
            logger.info(f"æ–‡æœ¬æ–‡æ¡£æ•°é‡: {len(text_docs)}")
            logger.info(f"å›¾åƒæ–‡æ¡£æ•°é‡: {len(image_docs)}")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. å¤„ç†æ–‡æœ¬æ–‡æ¡£
            if text_docs:
                print("==========")
                print("å¤„ç†æ–‡æœ¬æ–‡æ¡£å¼€å§‹")
                print("==========")
                logger.info("å¤„ç†æ–‡æœ¬æ–‡æ¡£çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å¤„ç†æ–‡æœ¬æ–‡æ¡£")
                self._process_text_documents(text_docs)
                logger.info("å¤„ç†æ–‡æœ¬æ–‡æ¡£æˆåŠŸ")
                print("å¤„ç†æ–‡æœ¬æ–‡æ¡£ç»“æŸ")
                print("==========")
            
            # 4. å¤„ç†å›¾åƒæ–‡æ¡£
            if image_docs:
                print("==========")
                print("å¤„ç†å›¾åƒæ–‡æ¡£å¼€å§‹")
                print("==========")
                logger.info("å¤„ç†å›¾åƒæ–‡æ¡£çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å¤„ç†å›¾åƒæ–‡æ¡£")
                self._process_image_documents(image_docs)
                logger.info("å¤„ç†å›¾åƒæ–‡æ¡£æˆåŠŸ")
                print("å¤„ç†å›¾åƒæ–‡æ¡£ç»“æŸ")
                print("==========")
            
            # 5. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæ·»åŠ ç»“æœ")
            logger.info(f"Successfully added {len(documents)} documents to RAG system")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ RAGæ–‡æ¡£æ·»åŠ æµç¨‹å®Œæˆ")
            print("=" * 50)
            logger.info("RAGæ–‡æ¡£æ·»åŠ æµç¨‹æˆåŠŸå®Œæˆ")
            
            return True
            
        except Exception as e:
            print("=" * 50)
            print("âŒ RAGæ–‡æ¡£æ·»åŠ æµç¨‹å¤±è´¥")
            print("=" * 50)
            logger.error(f"RAGæ–‡æ¡£æ·»åŠ æµç¨‹å¤±è´¥: {e}")
            logger.error(f"Error adding documents: {e}")
            return False
    
    def _process_text_documents(self, documents: List[Dict[str, Any]]):
        """å¤„ç†æ–‡æœ¬æ–‡æ¡£"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = []
            for doc in documents:
                content = doc.get('content', '')
                if content:
                    texts.append(content)
            
            if not texts:
                return
            
            # å‘é‡åŒ–æ–‡æœ¬
            vectors = self.vector_service.batch_text_to_vectors(texts)
            
            # æ·»åŠ åˆ°æ£€ç´¢ç³»ç»Ÿ
            self.retrieval_service.add_documents(vectors, documents)
            
            logger.info(f"Processed {len(texts)} text documents")
            
        except Exception as e:
            logger.error(f"Error processing text documents: {e}")
            raise
    
    def _process_image_documents(self, documents: List[Dict[str, Any]]):
        """å¤„ç†å›¾åƒæ–‡æ¡£"""
        try:
            # æå–å›¾åƒè·¯å¾„
            image_paths = []
            valid_docs = []
            
            for doc in documents:
                image_path = doc.get('image_path', '')
                if image_path and os.path.exists(image_path):
                    image_paths.append(image_path)
                    valid_docs.append(doc)
            
            if not image_paths:
                return
            
            # å‘é‡åŒ–å›¾åƒ
            vectors = self.vector_service.batch_image_to_vectors(image_paths)
            
            # æ·»åŠ åˆ°æ£€ç´¢ç³»ç»Ÿ
            self.retrieval_service.add_documents(vectors, valid_docs)
            
            logger.info(f"Processed {len(image_paths)} image documents")
            
        except Exception as e:
            logger.error(f"Error processing image documents: {e}")
            raise
    
    def query(self, question: str, top_k: int = 5, 
              response_type: str = "general", similarity_threshold: float = None) -> Dict[str, Any]:
        """
        æŸ¥è¯¢RAGç³»ç»Ÿ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            response_type: å“åº”ç±»å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            print("=" * 50)
            print("ğŸ” RAGæŸ¥è¯¢æµç¨‹å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹RAGæŸ¥è¯¢æµç¨‹...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šé—®é¢˜='{question}', top_k={top_k}, response_type='{response_type}'")
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹é¢„å¤„ç†ç”¨æˆ·é—®é¢˜")
            logger.info(f"é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            logger.info(f"é—®é¢˜ç±»å‹: {response_type}")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. ç”¨æˆ·æ•°æ®å‘é‡åŒ–
            print("==========")
            print("ç”¨æˆ·æ•°æ®å‘é‡åŒ–å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å‘é‡åŒ–çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡")
            query_vector = self.vector_service.text_to_vector(question)
            logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(query_vector) if query_vector is not None else 'None'}")
            logger.info("ç”¨æˆ·æ•°æ®å‘é‡åŒ–æˆåŠŸ")
            print("ç”¨æˆ·æ•°æ®å‘é‡åŒ–ç»“æŸ")
            print("==========")
            
            # 4. ç”¨æˆ·æ•°æ®æ£€ç´¢
            print("==========")
            print("ç”¨æˆ·æ•°æ®æ£€ç´¢å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®æ£€ç´¢çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£")
            logger.info(f"æ£€ç´¢å‚æ•°: top_k={top_k}")
            retrieved_docs = self.retrieval_service.retrieve_documents(
                query_vector, top_k=top_k, query_text=question
            )
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            logger.info("ç”¨æˆ·æ•°æ®æ£€ç´¢æˆåŠŸ")
            print("ç”¨æˆ·æ•°æ®æ£€ç´¢ç»“æŸ")
            print("==========")
            
            # 5. æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ
            print("==========")
            print("æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¼€å§‹")
            print("==========")
            logger.info("æ–‡æ¡£æ‘˜è¦ç”Ÿæˆçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œæ‘˜è¦")
            logger.info(f"å¾…æ‘˜è¦æ–‡æ¡£æ•°é‡: {len(retrieved_docs)}")
            
            # æ‰“å°åŸå§‹æ–‡æ¡£å†…å®¹
            for i, doc in enumerate(retrieved_docs, 1):
                title = doc.get('title', f'æ–‡æ¡£{i}')
                content = doc.get('content', '')
                similarity = doc.get('similarity', 0)
                logger.info(f"åŸå§‹æ–‡æ¡£{i}: æ ‡é¢˜='{title}', ç›¸ä¼¼åº¦={similarity:.3f}, å†…å®¹é•¿åº¦={len(content)}å­—ç¬¦")
                logger.info(f"åŸå§‹æ–‡æ¡£{i}å†…å®¹é¢„è§ˆ: {content[:100]}...")
            
            summary = self._summarize_documents(retrieved_docs, question)
            logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
            logger.info(f"ç”Ÿæˆçš„æ‘˜è¦å†…å®¹: {summary}")
            logger.info("æ–‡æ¡£æ‘˜è¦ç”ŸæˆæˆåŠŸ")
            print("æ–‡æ¡£æ‘˜è¦ç”Ÿæˆç»“æŸ")
            print("==========")
            
            # 6. æ„å»ºä¸Šä¸‹æ–‡
            print("==========")
            print("æ„å»ºä¸Šä¸‹æ–‡å¼€å§‹")
            print("==========")
            logger.info("æ„å»ºä¸Šä¸‹æ–‡çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹åŸºäºæ‘˜è¦æ„å»ºä¸Šä¸‹æ–‡")
            context = self._build_context(summary)
            logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"æ„å»ºçš„ä¸Šä¸‹æ–‡å†…å®¹: {context}")
            logger.info("æ„å»ºä¸Šä¸‹æ–‡æˆåŠŸ")
            print("æ„å»ºä¸Šä¸‹æ–‡ç»“æŸ")
            print("==========")
            
            # 7. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
            print("==========")
            print("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”å¼€å§‹")
            print("==========")
            logger.info("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹è°ƒç”¨LLMç”Ÿæˆå›ç­”")
            logger.info(f"LLMå‚æ•°: response_type='{response_type}'")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£ï¼Œå†³å®šç”Ÿæˆç­–ç•¥
            if retrieved_docs:
                logger.info("âœ… åŸºäºæ£€ç´¢åˆ°çš„åŒ»å­¦çŸ¥è¯†ç”Ÿæˆå›ç­”")
                answer = self.llm_service.generate_medical_response(
                    question, context, response_type
                )
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼Œä½¿ç”¨é€šç”¨å›ç­”æ¨¡å¼")
                answer = self.llm_service.generate_general_response(
                    question, response_type
                )
            logger.info(f"LLMç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
            logger.info("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”æˆåŠŸ")
            print("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ç»“æŸ")
            print("==========")
            
            # 8. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæœ€ç»ˆç»“æœ")
            result = {
                'question': question,
                'answer': answer,
                'context': context,
                'retrieved_documents': retrieved_docs,
                'response_type': response_type,
                'timestamp': datetime.now().isoformat()
            }
            logger.info("ç»“æœæ„å»ºå®Œæˆ")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ RAGæŸ¥è¯¢æµç¨‹å®Œæˆ")
            print("=" * 50)
            logger.info("RAGæŸ¥è¯¢æµç¨‹æˆåŠŸå®Œæˆ")
            
            return result
            
        except Exception as e:
            print("=" * 50)
            print("âŒ RAGæŸ¥è¯¢æµç¨‹å¤±è´¥")
            print("=" * 50)
            logger.error(f"RAGæŸ¥è¯¢æµç¨‹å¤±è´¥: {e}")
            logger.error(f"Error in RAG query: {e}")
            return {
                'question': question,
                'answer': 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚',
                'context': '',
                'retrieved_documents': [],
                'response_type': response_type,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    
    def _build_context(self, summary: str, max_length: int = 800) -> str:
        """
        åŸºäºæ‘˜è¦æ„å»ºä¸Šä¸‹æ–‡
        
        Args:
            summary: ç»Ÿä¸€æ‘˜è¦æ–‡æœ¬
            max_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        try:
            if not summary:
                return ""
            
            # æ„å»ºä¸Šä¸‹æ–‡æç¤ºè¯
            context = f"""åŸºäºä»¥ä¸‹åŒ»å­¦çŸ¥è¯†æ‘˜è¦ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

åŒ»å­¦çŸ¥è¯†æ‘˜è¦ï¼š
{summary}

è¯·åŸºäºä»¥ä¸ŠåŒ»å­¦çŸ¥è¯†ï¼Œç»“åˆæ‚¨çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚"""
            
            # å¦‚æœä¸Šä¸‹æ–‡å¤ªé•¿ï¼Œæˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if len(context) > max_length:
                context = context[:max_length] + "..."
            
            return context.strip()
            
        except Exception as e:
            logger.error(f"æ„å»ºä¸Šä¸‹æ–‡å‡ºé”™ï¼š{e}")
            return ""
    
    def chat(self, messages: List[Dict[str, str]], 
             top_k: int = 5) -> Dict[str, Any]:
        """
        å¯¹è¯åŠŸèƒ½
        
        Args:
            messages: å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            
        Returns:
            å¯¹è¯ç»“æœ
        """
        try:
            print("=" * 50)
            print("ğŸ’¬ RAGå¯¹è¯æµç¨‹å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹RAGå¯¹è¯æµç¨‹...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šå¯¹è¯å†å²é•¿åº¦={len(messages)}, top_k={top_k}")
            
            if not messages:
                logger.warning("å¯¹è¯å†å²ä¸ºç©º")
                return {
                    'answer': 'è¯·å‘Šè¯‰æˆ‘æ‚¨çš„é—®é¢˜ã€‚',
                    'context': '',
                    'retrieved_documents': [],
                    'timestamp': datetime.now().isoformat()
                }
            
            # è·å–æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            last_message = messages[-1]
            if last_message.get('role') != 'user':
                logger.warning("æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯")
                return {
                    'answer': 'è¯·å‘Šè¯‰æˆ‘æ‚¨çš„é—®é¢˜ã€‚',
                    'context': '',
                    'retrieved_documents': [],
                    'timestamp': datetime.now().isoformat()
                }
            
            question = last_message.get('content', '')
            logger.info(f"æå–ç”¨æˆ·é—®é¢˜: '{question}'")
            
            # ä¸å†è¿›è¡Œå…³é”®è¯åˆ¤æ–­ï¼Œè®©ç”Ÿæˆæ¨¡å‹æ ¹æ®æ£€ç´¢ç»“æœåˆ¤æ–­
            
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å¤„ç†å¯¹è¯å†å²")
            logger.info(f"å¯¹è¯è½®æ•°: {len(messages)}")
            logger.info(f"å½“å‰é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. ç”¨æˆ·æ•°æ®å‘é‡åŒ–
            print("==========")
            print("ç”¨æˆ·æ•°æ®å‘é‡åŒ–å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å‘é‡åŒ–çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡")
            query_vector = self.vector_service.text_to_vector(question)
            logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(query_vector) if query_vector is not None else 'None'}")
            logger.info("ç”¨æˆ·æ•°æ®å‘é‡åŒ–æˆåŠŸ")
            print("ç”¨æˆ·æ•°æ®å‘é‡åŒ–ç»“æŸ")
            print("==========")
            
            # 4. ç”¨æˆ·æ•°æ®æ£€ç´¢
            print("==========")
            print("ç”¨æˆ·æ•°æ®æ£€ç´¢å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®æ£€ç´¢çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£")
            logger.info(f"æ£€ç´¢å‚æ•°: top_k={top_k}")
            retrieved_docs = self.retrieval_service.retrieve_documents(
                query_vector, top_k=top_k, query_text=question
            )
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            logger.info("ç”¨æˆ·æ•°æ®æ£€ç´¢æˆåŠŸ")
            print("ç”¨æˆ·æ•°æ®æ£€ç´¢ç»“æŸ")
            print("==========")
            
            # 5. æ„å»ºä¸Šä¸‹æ–‡
            print("==========")
            print("æ„å»ºä¸Šä¸‹æ–‡å¼€å§‹")
            print("==========")
            logger.info("æ„å»ºä¸Šä¸‹æ–‡çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæ£€ç´¢æ–‡æ¡£çš„ä¸Šä¸‹æ–‡")
            context = self._build_context(retrieved_docs)
            logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info("æ„å»ºä¸Šä¸‹æ–‡æˆåŠŸ")
            print("æ„å»ºä¸Šä¸‹æ–‡ç»“æŸ")
            print("==========")
            
            # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
            print("==========")
            print("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”å¼€å§‹")
            print("==========")
            logger.info("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹è°ƒç”¨LLMç”Ÿæˆå¯¹è¯å›ç­”")
            logger.info(f"å¯¹è¯å†å²é•¿åº¦: {len(messages)}")
            answer = self.llm_service.chat_with_context(messages, context)
            logger.info(f"LLMç”Ÿæˆå®Œæˆï¼Œå›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
            logger.info("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”æˆåŠŸ")
            print("è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”ç»“æŸ")
            print("==========")
            
            # 7. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæœ€ç»ˆå¯¹è¯ç»“æœ")
            result = {
                'answer': answer,
                'context': context,
                'retrieved_documents': retrieved_docs,
                'timestamp': datetime.now().isoformat()
            }
            logger.info("ç»“æœæ„å»ºå®Œæˆ")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ RAGå¯¹è¯æµç¨‹å®Œæˆ")
            print("=" * 50)
            logger.info("RAGå¯¹è¯æµç¨‹æˆåŠŸå®Œæˆ")
            
            return result
            
        except Exception as e:
            print("=" * 50)
            print("âŒ RAGå¯¹è¯æµç¨‹å¤±è´¥")
            print("=" * 50)
            logger.error(f"RAGå¯¹è¯æµç¨‹å¤±è´¥: {e}")
            logger.error(f"Error in chat: {e}")
            return {
                'answer': 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚è¯·é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚',
                'context': '',
                'retrieved_documents': [],
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def batch_query(self, questions: List[str], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æŸ¥è¯¢
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            
        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        try:
            print("=" * 50)
            print("ğŸ“¦ RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šé—®é¢˜æ•°é‡={len(questions)}, top_k={top_k}")
            for i, question in enumerate(questions):
                logger.info(f"é—®é¢˜ {i+1}: '{question[:50]}{'...' if len(question) > 50 else ''}'")
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹é¢„å¤„ç†æ‰¹é‡é—®é¢˜")
            logger.info(f"æ€»é—®é¢˜æ•°: {len(questions)}")
            logger.info(f"å¹³å‡é—®é¢˜é•¿åº¦: {sum(len(q) for q in questions) / len(questions):.1f} å­—ç¬¦")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. æ‰¹é‡å¤„ç†
            print("==========")
            print("æ‰¹é‡å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("æ‰¹é‡å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹é€ä¸ªå¤„ç†é—®é¢˜")
            results = []
            
            for i, question in enumerate(questions):
                logger.info(f"å¤„ç†é—®é¢˜ {i+1}/{len(questions)}: '{question[:30]}{'...' if len(question) > 30 else ''}'")
                result = self.query(question, top_k)
                results.append(result)
                logger.info(f"é—®é¢˜ {i+1} å¤„ç†å®Œæˆ")
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªé—®é¢˜")
            logger.info("æ‰¹é‡å¤„ç†æˆåŠŸ")
            print("æ‰¹é‡å¤„ç†ç»“æŸ")
            print("==========")
            
            # 4. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæ‰¹é‡æŸ¥è¯¢ç»“æœ")
            logger.info(f"ç»“æœæ•°é‡: {len(results)}")
            success_count = sum(1 for r in results if 'error' not in r)
            logger.info(f"æˆåŠŸå¤„ç†: {success_count}/{len(results)} ä¸ªé—®é¢˜")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹å®Œæˆ")
            print("=" * 50)
            logger.info("RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹æˆåŠŸå®Œæˆ")
            
            return results
            
        except Exception as e:
            print("=" * 50)
            print("âŒ RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹å¤±è´¥")
            print("=" * 50)
            logger.error(f"RAGæ‰¹é‡æŸ¥è¯¢æµç¨‹å¤±è´¥: {e}")
            logger.error(f"Error in batch query: {e}")
            return []
    
    def search_documents(self, query: str, search_type: str = "semantic", 
                        top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            search_type: æœç´¢ç±»å‹ (semantic, keyword, hybrid)
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            print("=" * 50)
            print("ğŸ” RAGæ–‡æ¡£æœç´¢æµç¨‹å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹RAGæ–‡æ¡£æœç´¢æµç¨‹...")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            print("==========")
            print("è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹")
            print("==========")
            logger.info(f"è·å–ç”¨æˆ·è¾“å…¥ç»†èŠ‚æ—¥å¿—ï¼šæŸ¥è¯¢='{query}', æœç´¢ç±»å‹='{search_type}', top_k={top_k}")
            logger.info("è·å–ç”¨æˆ·è¾“å…¥æˆåŠŸ")
            print("è·å–ç”¨æˆ·è¾“å…¥ç»“æŸ")
            print("==========")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            print("==========")
            print("ç”¨æˆ·æ•°æ®å¤„ç†å¼€å§‹")
            print("==========")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹é¢„å¤„ç†æœç´¢æŸ¥è¯¢")
            logger.info(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
            logger.info(f"æœç´¢ç±»å‹: {search_type}")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†å®Œæˆ")
            logger.info("ç”¨æˆ·æ•°æ®å¤„ç†æˆåŠŸ")
            print("==========")
            
            # 3. æ‰§è¡Œæœç´¢
            print("==========")
            print("æ‰§è¡Œæœç´¢å¼€å§‹")
            print("==========")
            logger.info("æ‰§è¡Œæœç´¢çš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ‰§è¡Œæ–‡æ¡£æœç´¢")
            
            if search_type == "keyword":
                # å…³é”®è¯æœç´¢
                logger.info("ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")
                keywords = query.split()
                logger.info(f"æå–å…³é”®è¯: {keywords}")
                results = self.retrieval_service.search_by_keywords(keywords, top_k)
            else:
                # è¯­ä¹‰æœç´¢
                logger.info("ä½¿ç”¨è¯­ä¹‰æœç´¢æ¨¡å¼")
                logger.info("å¼€å§‹å‘é‡åŒ–æŸ¥è¯¢")
                query_vector = self.vector_service.text_to_vector(query)
                logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(query_vector) if query_vector is not None else 'None'}")
                results = self.retrieval_service.retrieve_documents(
                    query_vector, top_k=top_k, strategy=search_type, query_text=query
                )
            
            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            logger.info("æ‰§è¡Œæœç´¢æˆåŠŸ")
            print("æ‰§è¡Œæœç´¢ç»“æŸ")
            print("==========")
            
            # 4. è¿”å›ç”¨æˆ·ç»“æœ
            print("==========")
            print("è¿”å›ç”¨æˆ·ç»“æœå¼€å§‹")
            print("==========")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœçš„ç»†èŠ‚æ—¥å¿—ï¼šå¼€å§‹æ„å»ºæœç´¢ç»“æœ")
            logger.info(f"æœç´¢ç»“æœæ•°é‡: {len(results)}")
            logger.info("è¿”å›ç”¨æˆ·ç»“æœæˆåŠŸ")
            print("è¿”å›ç”¨æˆ·ç»“æœç»“æŸ")
            print("==========")
            
            print("=" * 50)
            print("ğŸ‰ RAGæ–‡æ¡£æœç´¢æµç¨‹å®Œæˆ")
            print("=" * 50)
            logger.info("RAGæ–‡æ¡£æœç´¢æµç¨‹æˆåŠŸå®Œæˆ")
            
            return results
            
        except Exception as e:
            print("=" * 50)
            print("âŒ RAGæ–‡æ¡£æœç´¢æµç¨‹å¤±è´¥")
            print("=" * 50)
            logger.error(f"RAGæ–‡æ¡£æœç´¢æµç¨‹å¤±è´¥: {e}")
            logger.error(f"Error searching documents: {e}")
            return []
    
    def get_system_stats(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        """
        try:
            vector_stats = self.vector_service.get_db_stats()
            retrieval_stats = self.retrieval_service.get_statistics()
            llm_info = self.llm_service.get_model_info()
            
            return {
                'vector_database': vector_stats,
                'retrieval_system': retrieval_stats,
                'llm_model': llm_info,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting system stats: {e}")
            return {}
    
    def save_system(self, save_dir: str):
        """
        ä¿å­˜ç³»ç»ŸçŠ¶æ€
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        try:
            os.makedirs(save_dir, exist_ok=True)
            
            # ä¿å­˜å‘é‡æ•°æ®åº“
            vector_db_path = os.path.join(save_dir, "vector_db.index")
            self.vector_service.save_vector_db(vector_db_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = os.path.join(save_dir, "metadata.json")
            self.retrieval_service.save_metadata(metadata_path)
            
            # ä¿å­˜é…ç½®
            config_path = os.path.join(save_dir, "config.json")
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"System saved to {save_dir}")
            
        except Exception as e:
            logger.error(f"Error saving system: {e}")
            raise
    
    def load_system(self, save_dir: str):
        """
        åŠ è½½ç³»ç»ŸçŠ¶æ€
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        try:
            # åŠ è½½å‘é‡æ•°æ®åº“
            vector_db_path = os.path.join(save_dir, "vector_db.index")
            if os.path.exists(vector_db_path):
                self.vector_service.load_vector_db(vector_db_path)
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = os.path.join(save_dir, "metadata.json")
            if os.path.exists(metadata_path):
                self.retrieval_service.load_metadata(metadata_path)
            
            logger.info(f"System loaded from {save_dir}")
            
        except Exception as e:
            logger.error(f"Error loading system: {e}")
            raise
    
    def clear_system(self):
        """æ¸…ç©ºç³»ç»Ÿæ•°æ®"""
        try:
            self.vector_service.clear_db()
            self.retrieval_service.clear_all()
            logger.info("System cleared")
        except Exception as e:
            logger.error(f"Error clearing system: {e}")
            raise
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
        except Exception as e:
            logger.error(f"Error in destructor: {e}")
    
    def _summarize_documents(self, documents: List[Dict[str, Any]], question: str) -> str:
        """
        å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•´åˆåç»Ÿä¸€ç”Ÿæˆæ‘˜è¦
        
        Args:
            documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æ•´åˆåçš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            logger.info("=" * 30)
            logger.info("ğŸ“ å¼€å§‹æ–‡æ¡£æ‘˜è¦ç”Ÿæˆæµç¨‹")
            logger.info("=" * 30)
            logger.info(f"è¾“å…¥å‚æ•°: æ–‡æ¡£æ•°é‡={len(documents)}, é—®é¢˜='{question}'")
            
            if not documents:
                logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦æ‘˜è¦ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
                return ""
            
            # æ•´åˆæ‰€æœ‰æ–‡æ¡£å†…å®¹
            logger.info("æ­¥éª¤1: å¼€å§‹æ•´åˆæ–‡æ¡£å†…å®¹")
            combined_content = ""
            for i, doc in enumerate(documents, 1):
                title = doc.get('title', f'æ–‡æ¡£{i}')
                content = doc.get('content', '')
                similarity = doc.get('similarity', 0)
                source = doc.get('source', 'æœªçŸ¥æ¥æº')
                
                logger.info(f"å¤„ç†æ–‡æ¡£{i}: æ ‡é¢˜='{title}', ç›¸ä¼¼åº¦={similarity:.3f}, æ¥æº='{source}'")
                logger.info(f"æ–‡æ¡£{i}åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.info(f"æ–‡æ¡£{i}å†…å®¹é¢„è§ˆ: {content[:150]}...")
                
                if content:
                    # åªæ·»åŠ çº¯å†…å®¹ï¼Œä¸åŒ…å«æ ¼å¼ä¿¡æ¯
                    combined_content += f"\n{content}\n"
                    logger.info(f"æ–‡æ¡£{i}å·²æ·»åŠ åˆ°åˆå¹¶å†…å®¹ä¸­")
                else:
                    logger.warning(f"æ–‡æ¡£{i}å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
            
            logger.info(f"æ–‡æ¡£æ•´åˆå®Œæˆï¼Œåˆå¹¶å†…å®¹æ€»é•¿åº¦: {len(combined_content)} å­—ç¬¦")
            logger.info(f"åˆå¹¶å†…å®¹é¢„è§ˆ: {combined_content[:300]}...")
            
            if not combined_content.strip():
                logger.warning("åˆå¹¶å†…å®¹ä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
                return ""
            
            # æ„å»ºç®€åŒ–æ‘˜è¦æç¤ºè¯
            logger.info("æ­¥éª¤2: å¼€å§‹æ„å»ºæ‘˜è¦æç¤ºè¯")
            # é™åˆ¶è¾“å…¥å†…å®¹é•¿åº¦ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
            max_input_length = 1000
            truncated_content = combined_content[:max_input_length]
            if len(combined_content) > max_input_length:
                logger.info(f"å†…å®¹è¿‡é•¿ï¼Œæˆªæ–­åˆ° {max_input_length} å­—ç¬¦")
            
            summary_prompt = f"""åŸºäºä»¥ä¸‹åŒ»å­¦çŸ¥è¯†ï¼Œç”Ÿæˆä¸è¶…è¿‡30å­—çš„ç®€æ´æ‘˜è¦ï¼Œé‡ç‚¹å›ç­”"{question}"ï¼š

{truncated_content}

è¦æ±‚ï¼šæ‘˜è¦å¿…é¡»æ§åˆ¶åœ¨30å­—ä»¥å†…ï¼Œç®€æ´æ˜äº†ã€‚
æ‘˜è¦ï¼š"""
            
            logger.info(f"æ‘˜è¦æç¤ºè¯æ„å»ºå®Œæˆ")
            logger.info(f"æç¤ºè¯æ€»é•¿åº¦: {len(summary_prompt)} å­—ç¬¦")
            logger.info(f"æç¤ºè¯å†…å®¹: {summary_prompt}")
            logger.info("æ­¥éª¤2: æ‘˜è¦æç¤ºè¯æ„å»ºå®Œæˆ")
            
            try:
                # ä¼˜å…ˆä½¿ç”¨æ–‡æœ¬æ‘˜è¦æœåŠ¡
                if self.summarization_service:
                    logger.info("æ­¥éª¤3: å¼€å§‹ä½¿ç”¨æ–‡æœ¬æ‘˜è¦æœåŠ¡è¿›è¡Œæ–‡æ¡£æ‘˜è¦")
                    summarization_config = self.config.get('summarization_service', {})
                    use_medical = summarization_config.get('use_medical_summarization', True)
                    medical_max_length = summarization_config.get('medical_max_length', 50)
                    
                    if use_medical:
                        summary = self.summarization_service.summarize_medical_text(
                            combined_content, 
                            max_length=medical_max_length
                        )
                        logger.info("ä½¿ç”¨åŒ»å­¦æ–‡æœ¬æ‘˜è¦æœåŠ¡å®Œæˆæ‘˜è¦")
                    else:
                        max_length = summarization_config.get('max_length', 100)
                        summary = self.summarization_service.summarize_text(
                            combined_content, 
                            max_length=max_length
                        )
                        logger.info("ä½¿ç”¨é€šç”¨æ–‡æœ¬æ‘˜è¦æœåŠ¡å®Œæˆæ‘˜è¦")
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šè°ƒç”¨LLMè¿›è¡Œç»Ÿä¸€æ‘˜è¦
                    logger.info("æ­¥éª¤3: å¼€å§‹è°ƒç”¨LLMè¿›è¡Œæ–‡æ¡£æ‘˜è¦ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
                    logger.info(f"LLMè°ƒç”¨å‚æ•°: max_new_tokens=100, temperature=0.3, timeout=600")
                    logger.info(f"å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯: {summary_prompt}")
                    
                    summary = self.llm_service.generate_text(
                        prompt=summary_prompt,
                        max_new_tokens=100,
                        temperature=0.3,
                        timeout=600  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º600ç§’
                    )
                
                logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆ")
                logger.info(f"åŸå§‹æ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
                logger.info(f"ç”Ÿæˆçš„åŸå§‹æ‘˜è¦: {summary}")
                
                # æ¸…ç†æ‘˜è¦å†…å®¹å¹¶æ§åˆ¶é•¿åº¦
                cleaned_summary = summary.strip()
                
                # å¼ºåˆ¶æ§åˆ¶æ‘˜è¦é•¿åº¦åœ¨30å­—ä»¥å†…
                if len(cleaned_summary) > 30:
                    logger.warning(f"æ‘˜è¦é•¿åº¦è¶…è¿‡30å­—é™åˆ¶ï¼Œå½“å‰é•¿åº¦: {len(cleaned_summary)} å­—ç¬¦")
                    # æˆªå–å‰30ä¸ªå­—ç¬¦
                    cleaned_summary = cleaned_summary[:30]
                    logger.info(f"å·²æˆªå–åˆ°30å­—: {cleaned_summary}")
                
                logger.info(f"æ¸…ç†åæ‘˜è¦é•¿åº¦: {len(cleaned_summary)} å­—ç¬¦")
                logger.info(f"æ¸…ç†åæ‘˜è¦å†…å®¹: {cleaned_summary}")
                
                logger.info(f"æ–‡æ¡£ç»Ÿä¸€æ‘˜è¦å®Œæˆï¼šæˆåŠŸæ•´åˆäº†{len(documents)}ä¸ªæ–‡æ¡£")
                logger.info("=" * 30)
                logger.info("ğŸ“ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆæµç¨‹å®Œæˆ")
                logger.info("=" * 30)
                
                return cleaned_summary
                
            except Exception as e:
                logger.error(f"LLMæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                logger.warning("ä½¿ç”¨å¤‡ç”¨æ‘˜è¦æ–¹æ¡ˆ")
                
                # å¦‚æœæ‘˜è¦å¤±è´¥ï¼Œè¿”å›å‰å‡ ä¸ªæ–‡æ¡£çš„ç®€è¦ä¿¡æ¯
                fallback_summary = ""
                for i, doc in enumerate(documents[:3], 1):
                    title = doc.get('title', f'æ–‡æ¡£{i}')
                    content = doc.get('content', '')[:100]
                    if content:
                        fallback_summary += f"ã€{title}ã€‘{content}...\n"
                        logger.info(f"å¤‡ç”¨æ‘˜è¦æ·»åŠ æ–‡æ¡£{i}: {title}")
                
                fallback_summary = fallback_summary.strip()
                logger.info(f"å¤‡ç”¨æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(fallback_summary)} å­—ç¬¦")
                logger.info(f"å¤‡ç”¨æ‘˜è¦å†…å®¹: {fallback_summary}")
                
                logger.info("=" * 30)
                logger.info("ğŸ“ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆæµç¨‹å®Œæˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
                logger.info("=" * 30)
                
                return fallback_summary
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ‘˜è¦è¿‡ç¨‹å‡ºé”™ï¼š{e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.info("=" * 30)
            logger.info("ğŸ“ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆæµç¨‹å¤±è´¥")
            logger.info("=" * 30)
            return ""
    
    async def query_stream(self, question: str, top_k: int = 5, 
                    response_type: str = "general", similarity_threshold: float = None):
        """
        æµå¼æŸ¥è¯¢RAGç³»ç»Ÿï¼Œé€æ­¥ç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            response_type: å“åº”ç±»å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            
        Yields:
            æµå¼å“åº”æ•°æ®
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸ” RAGæµå¼æŸ¥è¯¢æµç¨‹å¼€å§‹")
            logger.info("=" * 50)
            logger.info("å¼€å§‹RAGæµå¼æŸ¥è¯¢æµç¨‹...")
            logger.info(f"æŸ¥è¯¢å‚æ•°: question='{question}', top_k={top_k}, response_type='{response_type}'")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            yield {"type": "status", "message": "æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜..."}
            logger.info("æ­¥éª¤1: è·å–ç”¨æˆ·è¾“å…¥å®Œæˆ")
            
            # 2. ç”¨æˆ·æ•°æ®å‘é‡åŒ–
            yield {"type": "status", "message": "æ­£åœ¨åˆ†æé—®é¢˜..."}
            logger.info("æ­¥éª¤2: å¼€å§‹ç”¨æˆ·æ•°æ®å‘é‡åŒ–")
            logger.info(f"é—®é¢˜å†…å®¹: '{question}'")
            logger.info(f"é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            
            query_vector = self.vector_service.text_to_vector(question)
            logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {len(query_vector) if query_vector is not None else 'None'}")
            logger.info("æ­¥éª¤2: ç”¨æˆ·æ•°æ®å‘é‡åŒ–å®Œæˆ")
            
            # 3. ç”¨æˆ·æ•°æ®æ£€ç´¢
            yield {"type": "status", "message": "æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†..."}
            logger.info("æ­¥éª¤3: å¼€å§‹ç”¨æˆ·æ•°æ®æ£€ç´¢")
            logger.info(f"æ£€ç´¢å‚æ•°: top_k={top_k}, query_text='{question}'")
            
            retrieved_docs = self.retrieval_service.retrieve_documents(
                query_vector, top_k=top_k, query_text=question
            )
            
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            # è¯¦ç»†è®°å½•æ£€ç´¢ç»“æœ
            for i, doc in enumerate(retrieved_docs, 1):
                title = doc.get('title', f'æ–‡æ¡£{i}')
                content = doc.get('content', '')
                similarity = doc.get('similarity', 0)
                source = doc.get('source', 'æœªçŸ¥æ¥æº')
                logger.info(f"æ£€ç´¢ç»“æœ{i}: æ ‡é¢˜='{title}', ç›¸ä¼¼åº¦={similarity:.3f}, æ¥æº='{source}'")
                logger.info(f"æ£€ç´¢ç»“æœ{i}å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.info(f"æ£€ç´¢ç»“æœ{i}å†…å®¹é¢„è§ˆ: {content[:200]}...")
            
            logger.info("æ­¥éª¤3: ç”¨æˆ·æ•°æ®æ£€ç´¢å®Œæˆ")
            
            # 4. å†…å®¹æ‘˜è¦
            yield {"type": "status", "message": "æ­£åœ¨æ•´ç†æ£€ç´¢åˆ°çš„å†…å®¹..."}
            logger.info("æ­¥éª¤4: å¼€å§‹å†…å®¹æ‘˜è¦ç”Ÿæˆ")
            logger.info(f"å¾…æ‘˜è¦æ–‡æ¡£æ•°é‡: {len(retrieved_docs)}")
            
            summary = self._summarize_documents(retrieved_docs, question)
            logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
            logger.info(f"ç”Ÿæˆçš„æ‘˜è¦å†…å®¹: {summary}")
            logger.info("æ­¥éª¤4: å†…å®¹æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            
            # 5. æ„å»ºä¸Šä¸‹æ–‡
            logger.info("æ­¥éª¤5: å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡")
            context = self._build_context(summary)
            logger.info(f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"æ„å»ºçš„ä¸Šä¸‹æ–‡å†…å®¹: {context}")
            logger.info("æ­¥éª¤5: æ„å»ºä¸Šä¸‹æ–‡å®Œæˆ")
            
            # 6. æµå¼ç”Ÿæˆå›ç­”
            yield {"type": "status", "message": "æ­£åœ¨ç”Ÿæˆå›ç­”..."}
            logger.info("æ­¥éª¤6: å¼€å§‹æµå¼ç”Ÿæˆå›ç­”")
            logger.info(f"LLMå‚æ•°: response_type='{response_type}'")
            logger.info(f"å‘é€ç»™LLMçš„å®Œæ•´ä¸Šä¸‹æ–‡: {context}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£ï¼Œå†³å®šç”Ÿæˆç­–ç•¥
            if retrieved_docs:
                logger.info("âœ… åŸºäºæ£€ç´¢åˆ°çš„åŒ»å­¦çŸ¥è¯†ç”Ÿæˆå›ç­”")
                logger.info("è°ƒç”¨LLMæœåŠ¡: generate_medical_response_stream")
                
                answer_chunks = []
                for chunk in self.llm_service.generate_medical_response_stream(
                    question, context, response_type
                ):
                    answer_chunks.append(chunk)
                    yield {"type": "content", "content": chunk}
                
                full_answer = ''.join(answer_chunks)
                logger.info(f"LLMæµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»å›ç­”é•¿åº¦: {len(full_answer)} å­—ç¬¦")
                logger.info(f"LLMç”Ÿæˆçš„å®Œæ•´å›ç­”: {full_answer}")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼Œä½¿ç”¨é€šç”¨å›ç­”æ¨¡å¼")
                logger.info("è°ƒç”¨LLMæœåŠ¡: generate_general_response_stream")
                
                answer_chunks = []
                for chunk in self.llm_service.generate_general_response_stream(
                    question, response_type
                ):
                    answer_chunks.append(chunk)
                    yield {"type": "content", "content": chunk}
                
                full_answer = ''.join(answer_chunks)
                logger.info(f"LLMæµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»å›ç­”é•¿åº¦: {len(full_answer)} å­—ç¬¦")
                logger.info(f"LLMç”Ÿæˆçš„å®Œæ•´å›ç­”: {full_answer}")
            
            logger.info("æ­¥éª¤6: æµå¼ç”Ÿæˆå›ç­”å®Œæˆ")
            
            # 7. è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯
            logger.info("æ­¥éª¤7: å¼€å§‹è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯")
            documents_info = [
                {
                    "title": doc.get('title', ''),
                    "content": doc.get('content', ''),
                    "similarity": doc.get('similarity', 0),
                    "source": doc.get('source', '')
                }
                for doc in retrieved_docs
            ]
            
            logger.info(f"è¿”å›æ–‡æ¡£ä¿¡æ¯æ•°é‡: {len(documents_info)}")
            for i, doc_info in enumerate(documents_info, 1):
                logger.info(f"è¿”å›æ–‡æ¡£{i}: æ ‡é¢˜='{doc_info['title']}', ç›¸ä¼¼åº¦={doc_info['similarity']:.3f}")
            
            yield {
                "type": "documents", 
                "documents": documents_info
            }
            
            logger.info("æ­¥éª¤7: è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯å®Œæˆ")
            
            yield {"type": "complete", "message": "å›ç­”ç”Ÿæˆå®Œæˆ"}
            logger.info("ğŸ‰ RAGæµå¼æŸ¥è¯¢æµç¨‹å®Œæˆ")
            logger.info("=" * 50)
            
        except Exception as e:
            logger.error(f"æµå¼æŸ¥è¯¢å‡ºé”™ï¼š{e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            yield {"type": "error", "message": f"æŸ¥è¯¢å‡ºé”™ï¼š{str(e)}"}


class RAGPipelineFactory:
    """RAGæµç¨‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_rag_pipeline(config_path: str = None) -> RAGPipeline:
        """
        åˆ›å»ºRAGæµç¨‹å®ä¾‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            RAGæµç¨‹å®ä¾‹
        """
        # é»˜è®¤é…ç½®
        default_config = {
            'vector_service': {
                'device': 'cuda',
                'vector_dim': 384,
                'batch_size': 32
            },
            'retrieval_service': {
                'vector_dim': 384,
                'max_results': 20,
                'similarity_threshold': 0.7,
                'retrieval_strategy': 'semantic'
            },
            'llm_service': {
                'device': 'cuda',
                'model_path': 'ai_models/llm_models/Qwen2-0.5B-Medical-MLX',
                'max_length': 2048,
                'temperature': 0.7
            }
        }
        
        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½é…ç½®
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"Error loading config file: {e}, using default config")
        
        return RAGPipeline(default_config)


if __name__ == "__main__":
    # æµ‹è¯•RAGæµç¨‹
    config = {
        'vector_service': {
            'device': 'cpu',
            'vector_dim': 384,
            'batch_size': 16
        },
        'retrieval_service': {
            'vector_dim': 384,
            'max_results': 10,
            'similarity_threshold': 0.5,
            'retrieval_strategy': 'semantic'
        },
        'llm_service': {
            'device': 'cpu',
            'model_path': 'ai_models/llm_models/Qwen2-0.5B-Medical-MLX',
            'max_length': 1024,
            'temperature': 0.7
        }
    }
    
    try:
        # åˆ›å»ºRAGæµç¨‹
        rag_pipeline = RAGPipeline(config)
        
        # æµ‹è¯•æ–‡æ¡£
        test_documents = [
            {
                'content': 'å¿ƒè‚Œæ¢—æ­»æ˜¯ç”±äºå† çŠ¶åŠ¨è„‰æ€¥æ€§é—­å¡å¯¼è‡´å¿ƒè‚Œç¼ºè¡€åæ­»çš„å¿ƒè¡€ç®¡ç–¾ç—…ã€‚',
                'title': 'å¿ƒè‚Œæ¢—æ­»å®šä¹‰',
                'category': 'å¿ƒè¡€ç®¡ç–¾ç—…',
                'source': 'åŒ»å­¦æ•™ç§‘ä¹¦'
            },
            {
                'content': 'èƒ¸ç—›æ˜¯å¿ƒè‚Œæ¢—æ­»æœ€å¸¸è§çš„ç—‡çŠ¶ï¼Œé€šå¸¸è¡¨ç°ä¸ºèƒ¸éª¨åå‹æ¦¨æ€§ç–¼ç—›ã€‚',
                'title': 'å¿ƒè‚Œæ¢—æ­»ç—‡çŠ¶',
                'category': 'å¿ƒè¡€ç®¡ç–¾ç—…',
                'source': 'ä¸´åºŠæŒ‡å—'
            },
            {
                'content': 'å¿ƒç”µå›¾æ£€æŸ¥æ˜¯è¯Šæ–­å¿ƒè‚Œæ¢—æ­»çš„é‡è¦æ–¹æ³•ï¼Œå¯æ˜¾ç¤ºSTæ®µæŠ¬é«˜æˆ–å‹ä½ã€‚',
                'title': 'å¿ƒè‚Œæ¢—æ­»è¯Šæ–­',
                'category': 'å¿ƒè¡€ç®¡ç–¾ç—…',
                'source': 'è¯Šæ–­æ‰‹å†Œ'
            }
        ]
        
        # æ·»åŠ æ–‡æ¡£
        success = rag_pipeline.add_documents(test_documents)
        print(f"Documents added: {success}")
        
        # æµ‹è¯•æŸ¥è¯¢
        question = "ä»€ä¹ˆæ˜¯å¿ƒè‚Œæ¢—æ­»ï¼Ÿ"
        result = rag_pipeline.query(question, top_k=2)
        print(f"\nQuestion: {question}")
        print(f"Answer: {result['answer']}")
        print(f"Retrieved documents: {len(result['retrieved_documents'])}")
        
        # æµ‹è¯•å¯¹è¯
        messages = [
            {"role": "user", "content": "å¿ƒè‚Œæ¢—æ­»æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ"}
        ]
        chat_result = rag_pipeline.chat(messages)
        print(f"\nChat answer: {chat_result['answer']}")
        
        # è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        stats = rag_pipeline.get_system_stats()
        print(f"\nSystem stats: {stats}")
        
    except Exception as e:
        print(f"Error in testing: {e}")
        print("Please ensure all services are properly configured.")


