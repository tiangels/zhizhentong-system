"""
å¤§è¯­è¨€æ¨¡å‹æœåŠ¡æ¨¡å—
è´Ÿè´£è°ƒç”¨Qwen2-0.5B-Medical-MLXæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
"""

import os
import json
import logging
import torch
import threading
import time
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TimeoutException(Exception):
    """è¶…æ—¶å¼‚å¸¸"""
    pass


class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ç±»ï¼Œè´Ÿè´£æ–‡æœ¬ç”Ÿæˆ"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹è·¯å¾„ã€è®¾å¤‡ç­‰é…ç½®ä¿¡æ¯
        """
        self.config = config
        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        self.model_path = config.get('model_path', '/Users/tiangels/AI/llm_learning_project/zhi_zhen_tong_system/codes/ai_models/llm_models/Qwen3-1.7b-Medical-R1-sft')
        self.max_length = config.get('max_length', 2048)
        self.temperature = config.get('temperature', 0.7)
        self.top_p = config.get('top_p', 0.9)
        self.top_k = config.get('top_k', 50)
        self.repetition_penalty = config.get('repetition_penalty', 1.1)
        
        # æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model = None
        self.tokenizer = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½å¤§è¯­è¨€æ¨¡å‹"""
        try:
            logger.info(f"Loading Qwen2 Medical model from {self.model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒæœ¬åœ°è·¯å¾„å’ŒHugging Faceæ¨¡å‹åç§°ï¼‰
            is_huggingface_model = "/" in self.model_path and not os.path.exists(self.model_path)
            if not is_huggingface_model and not os.path.exists(self.model_path):
                logger.error(f"Model path does not exist: {self.model_path}")
                raise FileNotFoundError(f"Model path does not exist: {self.model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            logger.info("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            logger.info("Loading model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16 if self.device == 'cuda' else torch.float32,
                device_map="auto" if self.device == 'cuda' else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            
            # å¦‚æœä½¿ç”¨CPUï¼Œå°†æ¨¡å‹ç§»åŠ¨åˆ°CPU
            if self.device == 'cpu':
                self.model = self.model.to(self.device)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("Qwen2 Medical model loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def generate_text(self, prompt: str, max_new_tokens: int = 128, 
                     temperature: float = None, top_p: float = None,
                     top_k: int = None, repetition_penalty: float = None,
                     timeout: int = 600) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦è¶…æ—¶å¤„ç†ï¼‰
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆå‡å°‘åˆ°256ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            top_k: top_kå‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½šå‚æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        result = [None]
        exception = [None]
        
        def generate_worker():
            """åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆä»»åŠ¡"""
            try:
                # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é»˜è®¤å‚æ•°
                gen_temperature = temperature or self.temperature
                gen_top_p = top_p or self.top_p
                gen_top_k = top_k or self.top_k
                gen_repetition_penalty = repetition_penalty or self.repetition_penalty
                
                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer.encode(prompt, return_tensors="pt")
                if inputs.size(1) > self.max_length:
                    inputs = inputs[:, -self.max_length:]
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                inputs = inputs.to(self.device)
                
                # ç”Ÿæˆé…ç½®ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
                generation_config = GenerationConfig(
                    max_new_tokens=max_new_tokens,
                    temperature=gen_temperature,
                    top_p=gen_top_p,
                    top_k=gen_top_k,
                    repetition_penalty=gen_repetition_penalty,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    bos_token_id=self.tokenizer.bos_token_id,
                    use_cache=True
                )
                
                # ç”Ÿæˆæ–‡æœ¬
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        generation_config=generation_config,
                        attention_mask=torch.ones_like(inputs)
                    )
                
                # è§£ç è¾“å‡º
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                result[0] = generated_text
                
            except Exception as e:
                exception[0] = e
        
        try:
            # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
            thread = threading.Thread(target=generate_worker)
            thread.daemon = True
            thread.start()
            
            # ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
            thread.join(timeout=timeout)
            
            if thread.is_alive():
                # è¶…æ—¶
                logger.warning(f"LLM generation timeout after {timeout} seconds")
                return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            
            if exception[0]:
                # æœ‰å¼‚å¸¸
                raise exception[0]
            
            if result[0] is None:
                return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
            
            return result[0]
            
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def generate_general_response(self, query: str, response_type: str = "general") -> str:
        """
        ç”Ÿæˆé€šç”¨å“åº”ï¼ˆå½“æ²¡æœ‰æ‰¾åˆ°ç›¸å…³åŒ»å­¦çŸ¥è¯†æ—¶ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            response_type: å“åº”ç±»å‹
            
        Returns:
            é€šç”¨å“åº”æ–‡æœ¬
        """
        try:
            logger.info("å¼€å§‹LLMé€šç”¨å“åº”ç”Ÿæˆ...")
            
            # æ„å»ºé€šç”¨æç¤ºè¯
            prompt = self._build_general_fallback_prompt(query, response_type)
            logger.info(f"é€šç”¨æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ç”Ÿæˆå“åº”
            response = self.generate_text(
                prompt,
                max_new_tokens=512,
                temperature=0.3,  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ç¡®ä¿å›ç­”æ›´å‡†ç¡®
                timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’
            )
            
            logger.info(f"LLMé€šç”¨å“åº”ç”Ÿæˆå®Œæˆï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            return response
            
        except Exception as e:
            logger.error(f"LLMé€šç”¨å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®ä¿¡æ¯ã€‚"
    
    def generate_medical_response(self, query: str, context: str = "", 
                                response_type: str = "diagnosis") -> str:
        """
        ç”ŸæˆåŒ»ç–—å“åº”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            response_type: å“åº”ç±»å‹ (diagnosis, advice, explanation)
            
        Returns:
            åŒ»ç–—å“åº”æ–‡æœ¬
        """
        try:
            logger.info("=" * 40)
            logger.info("ğŸ¤– å¼€å§‹LLMåŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹")
            logger.info("=" * 40)
            logger.info(f"è¾“å…¥å‚æ•°: query='{query}', contexté•¿åº¦={len(context)}, response_type='{response_type}'")
            
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            logger.info("æ­¥éª¤1: è·å–ç”¨æˆ·è¾“å…¥")
            logger.info(f"ç”¨æˆ·æŸ¥è¯¢: '{query}'")
            logger.info(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
            logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"å“åº”ç±»å‹: {response_type}")
            logger.info(f"å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹: {context}")
            logger.info("æ­¥éª¤1: è·å–ç”¨æˆ·è¾“å…¥å®Œæˆ")
            
            # 2. ç”¨æˆ·æ•°æ®å¤„ç†
            logger.info("æ­¥éª¤2: å¼€å§‹æ„å»ºåŒ»ç–—æç¤ºè¯")
            logger.info(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
            logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"å“åº”ç±»å‹: {response_type}")
            
            # æ„å»ºåŒ»ç–—æç¤ºè¯
            if response_type == "diagnosis":
                prompt = self._build_diagnosis_prompt(query, context)
                logger.info("æ„å»ºè¯Šæ–­æç¤ºè¯")
            elif response_type == "advice":
                prompt = self._build_advice_prompt(query, context)
                logger.info("æ„å»ºå»ºè®®æç¤ºè¯")
            elif response_type == "explanation":
                prompt = self._build_explanation_prompt(query, context)
                logger.info("æ„å»ºè§£é‡Šæç¤ºè¯")
            else:
                prompt = self._build_general_prompt(query, context)
                logger.info("æ„å»ºé€šç”¨æç¤ºè¯")
            
            logger.info(f"æç¤ºè¯æ„å»ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"å®Œæ•´æç¤ºè¯å†…å®¹: {prompt}")
            logger.info("æ­¥éª¤2: æ„å»ºåŒ»ç–—æç¤ºè¯å®Œæˆ")
            
            # 3. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
            logger.info("æ­¥éª¤3: å¼€å§‹è°ƒç”¨LLMç”ŸæˆåŒ»ç–—å“åº”")
            logger.info(f"LLMè°ƒç”¨å‚æ•°: max_new_tokens=128, temperature=0.7, timeout=600")
            logger.info(f"å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯: {prompt}")
            
            # ç”Ÿæˆå“åº”
            response = self.generate_text(
                prompt,
                max_new_tokens=128,  # å‡å°‘tokenæ•°é‡
                temperature=0.7,
                timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’
            )
            
            logger.info(f"LLMç”Ÿæˆå®Œæˆ")
            logger.info(f"åŸå§‹å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            logger.info(f"LLMç”Ÿæˆçš„åŸå§‹å“åº”: {response}")
            
            # æ¸…ç†å“åº”å†…å®¹
            cleaned_response = response.strip()
            logger.info(f"æ¸…ç†åå“åº”é•¿åº¦: {len(cleaned_response)} å­—ç¬¦")
            logger.info(f"æ¸…ç†åå“åº”å†…å®¹: {cleaned_response}")
            
            logger.info("æ­¥éª¤3: è°ƒç”¨LLMç”ŸæˆåŒ»ç–—å“åº”å®Œæˆ")
            logger.info("=" * 40)
            logger.info("ğŸ¤– LLMåŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹å®Œæˆ")
            logger.info("=" * 40)
            
            return cleaned_response
            
        except Exception as e:
            logger.error(f"LLMåŒ»ç–—å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.info("=" * 40)
            logger.info("ğŸ¤– LLMåŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹å¤±è´¥")
            logger.info("=" * 40)
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”ŸæˆåŒ»ç–—å»ºè®®ã€‚è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚"
    
    def _build_diagnosis_prompt(self, query: str, context: str) -> str:
        """æ„å»ºè¯Šæ–­æç¤ºè¯"""
        prompt = f"""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›è¯Šæ–­å»ºè®®ï¼š
        ç”¨æˆ·ç—‡çŠ¶æè¿°ï¼š{query}

        ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š
        {context}

        è¯·æä¾›ï¼š
        1. å¯èƒ½çš„ç–¾ç—…è¯Šæ–­
        2. è¯Šæ–­ä¾æ®
        3. å»ºè®®çš„æ£€æŸ¥é¡¹ç›®
        4. æ³¨æ„äº‹é¡¹

        æ³¨æ„ï¼šè¿™ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­ã€‚"""
        
        return prompt
    
    def _build_advice_prompt(self, query: str, context: str) -> str:
        """æ„å»ºå»ºè®®æç¤ºè¯"""
        prompt = f"""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›å¥åº·å»ºè®®ï¼š

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š
        {context}

        è¯·æä¾›ï¼š
        1. å¥åº·å»ºè®®
        2. é¢„é˜²æªæ–½
        3. ç”Ÿæ´»æ–¹å¼å»ºè®®
        4. ä½•æ—¶éœ€è¦å°±åŒ»

        æ³¨æ„ï¼šè¿™ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„å»ºè®®ã€‚"""
        
        return prompt
    
    def _build_explanation_prompt(self, query: str, context: str) -> str:
        """æ„å»ºè§£é‡Šæç¤ºè¯"""
        prompt = f"""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œè¯·è§£é‡Šä»¥ä¸‹åŒ»å­¦æ¦‚å¿µï¼š

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š
        {context}

        è¯·æä¾›ï¼š
        1. è¯¦ç»†è§£é‡Š
        2. ç›¸å…³æœºåˆ¶
        3. ä¸´åºŠè¡¨ç°
        4. æ²»ç–—åŸåˆ™

        æ³¨æ„ï¼šè¿™ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è§£é‡Šã€‚"""
        
        return prompt
    
    def _build_general_prompt(self, query: str, context: str) -> str:
        """æ„å»ºé€šç”¨æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œå…·æœ‰ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„åŒ»å­¦çŸ¥è¯†ï¼Œç»™å‡ºä¸“ä¸šã€å‡†ç¡®ã€å®ç”¨çš„åŒ»ç–—å»ºè®®ã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š
        {context}

        è¯·æŒ‰ç…§ä»¥ä¸‹ä¸“ä¸šæ ¼å¼å›ç­”ï¼š

        ã€ç—‡çŠ¶åˆ†æã€‘
        - æ ¹æ®ç”¨æˆ·æè¿°ï¼Œåˆ†æå¯èƒ½çš„ç—‡çŠ¶ç‰¹ç‚¹
        - ç»“åˆåŒ»å­¦çŸ¥è¯†ï¼Œè¯´æ˜ç—‡çŠ¶çš„å¯èƒ½åŸå› 

        ã€ä¸“ä¸šå»ºè®®ã€‘
        - æä¾›å…·ä½“çš„åŒ»ç–—å»ºè®®å’Œæ³¨æ„äº‹é¡¹
        - å»ºè®®å¿…è¦çš„æ£€æŸ¥æˆ–è§‚å¯Ÿè¦ç‚¹
        - ç»™å‡ºç”Ÿæ´»è°ƒç†å»ºè®®

        ã€å°±åŒ»æŒ‡å¯¼ã€‘
        - æ˜ç¡®ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦åŠæ—¶å°±åŒ»
        - å»ºè®®å°±è¯Šç§‘å®¤å’Œæ£€æŸ¥é¡¹ç›®
        - æé†’ç´§æ€¥æƒ…å†µçš„å¤„ç†æ–¹å¼

        ã€æ³¨æ„äº‹é¡¹ã€‘
        - å¼ºè°ƒæ­¤å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
        - æé†’ç”¨æˆ·åŠæ—¶å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ

        è¦æ±‚ï¼š
        - è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé¿å…è¿‡äºå¤æ‚çš„åŒ»å­¦æœ¯è¯­
        - å›ç­”è¦å…·ä½“å®ç”¨ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
        - åŸºäºæä¾›çš„åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
        - ä¿æŒåŒ»ç–—å»ºè®®çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§"""
        
        return prompt
    
    def chat_with_context(self, messages: List[Dict[str, str]], 
                         context: str = "") -> str:
        """
        åŸºäºä¸Šä¸‹æ–‡çš„å¯¹è¯
        
        Args:
            messages: å¯¹è¯å†å²
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            å›å¤æ–‡æœ¬
        """
        try:
            # æ„å»ºå¯¹è¯æç¤ºè¯
            prompt = self._build_chat_prompt(messages, context)
            
            # ç”Ÿæˆå›å¤
            response = self.generate_text(
                prompt,
                max_new_tokens=1024,  # å¢åŠ ç”Ÿæˆé•¿åº¦ï¼Œæ”¯æŒæ›´è¯¦ç»†çš„å›ç­”
                temperature=0.3,      # é™ä½æ¸©åº¦ï¼Œæé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
                top_p=0.8,           # è°ƒæ•´top_pï¼Œæé«˜å›ç­”è´¨é‡
                repetition_penalty=1.1  # é¿å…é‡å¤
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error in chat with context: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚è¯·é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚"
    
    def _build_chat_prompt(self, messages: List[Dict[str, str]], context: str) -> str:
        """æ„å»ºå¯¹è¯æç¤ºè¯"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œå…·æœ‰ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚

        """
        
        # æ·»åŠ åŒ»å­¦çŸ¥è¯†ä¸Šä¸‹æ–‡
        if context:
            prompt += f"ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š\n{context}\n\n"
            prompt += """è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„åŒ»å­¦çŸ¥è¯†ï¼Œç»™å‡ºä¸“ä¸šã€å‡†ç¡®ã€å®ç”¨çš„åŒ»ç–—å»ºè®®ã€‚

            è¯·æŒ‰ç…§ä»¥ä¸‹ä¸“ä¸šæ ¼å¼å›ç­”ï¼š

            ã€ç—‡çŠ¶åˆ†æã€‘
            - æ ¹æ®ç”¨æˆ·æè¿°ï¼Œåˆ†æå¯èƒ½çš„ç—‡çŠ¶ç‰¹ç‚¹
            - ç»“åˆåŒ»å­¦çŸ¥è¯†ï¼Œè¯´æ˜ç—‡çŠ¶çš„å¯èƒ½åŸå› 

            ã€ä¸“ä¸šå»ºè®®ã€‘
            - æä¾›å…·ä½“çš„åŒ»ç–—å»ºè®®å’Œæ³¨æ„äº‹é¡¹
            - å»ºè®®å¿…è¦çš„æ£€æŸ¥æˆ–è§‚å¯Ÿè¦ç‚¹
            - ç»™å‡ºç”Ÿæ´»è°ƒç†å»ºè®®

            ã€å°±åŒ»æŒ‡å¯¼ã€‘
            - æ˜ç¡®ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦åŠæ—¶å°±åŒ»
            - å»ºè®®å°±è¯Šç§‘å®¤å’Œæ£€æŸ¥é¡¹ç›®
            - æé†’ç´§æ€¥æƒ…å†µçš„å¤„ç†æ–¹å¼

            ã€æ³¨æ„äº‹é¡¹ã€‘
            - å¼ºè°ƒæ­¤å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
            - æé†’ç”¨æˆ·åŠæ—¶å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ

            è¦æ±‚ï¼š
            - è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé¿å…è¿‡äºå¤æ‚çš„åŒ»å­¦æœ¯è¯­
            - å›ç­”è¦å…·ä½“å®ç”¨ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
            - åŸºäºæä¾›çš„åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
            - ä¿æŒåŒ»ç–—å»ºè®®çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§"""
        else:
            prompt += """è¯·æ³¨æ„ï¼šæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„åŒ»å­¦çŸ¥è¯†ã€‚

            è¯·æ ¹æ®ä»¥ä¸‹æƒ…å†µåˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„æ€§è´¨ï¼š

            1. å¦‚æœç”¨æˆ·åªæ˜¯ç®€å•çš„é—®å€™ï¼ˆå¦‚"ä½ å¥½"ã€"æ‚¨å¥½"ç­‰ï¼‰ï¼Œè¯·å‹å¥½åœ°å›åº”å¹¶å¼•å¯¼ç”¨æˆ·æè¿°å…·ä½“çš„å¥åº·é—®é¢˜ã€‚

            2. å¦‚æœç”¨æˆ·è¯¢é—®çš„æ˜¯åŒ»ç–—ç›¸å…³é—®é¢˜ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼Œè¯·ï¼š
            - ç†è§£ç”¨æˆ·çš„é—®é¢˜å’Œå…³æ³¨ç‚¹
            - æä¾›ä¸€èˆ¬æ€§çš„å¥åº·å»ºè®®
            - å¼ºçƒˆå»ºè®®ç”¨æˆ·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
            - å¼ºè°ƒæ­¤å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­

            3. å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸æ˜ç¡®ï¼Œè¯·è¯¢é—®æ›´å¤šç»†èŠ‚ä»¥ä¾¿æä¾›æ›´å¥½çš„å¸®åŠ©ã€‚

            è¦æ±‚ï¼š
            - è¯­è¨€æ¸©å’Œã€ä¸“ä¸š
            - é¿å…ç»™å‡ºå…·ä½“çš„è¯Šæ–­æˆ–æ²»ç–—å»ºè®®
            - é‡ç‚¹å¼ºè°ƒå’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿçš„é‡è¦æ€§
            - ä¿æŒå›ç­”çš„è°¨æ…æ€§å’Œå®‰å…¨æ€§"""
        
        # æ·»åŠ å¯¹è¯å†å²
        prompt += "\n\nå¯¹è¯å†å²ï¼š\n"
        for message in messages[-6:]:  # åªä¿ç•™æœ€è¿‘6è½®å¯¹è¯
            role = "ç”¨æˆ·" if message["role"] == "user" else "åŠ©æ‰‹"
            prompt += f"{role}ï¼š{message['content']}\n"
        
        return prompt
    
    def _build_general_fallback_prompt(self, query: str, response_type: str) -> str:
        """æ„å»ºé€šç”¨å›é€€æç¤ºè¯ï¼ˆå½“æ²¡æœ‰æ‰¾åˆ°ç›¸å…³åŒ»å­¦çŸ¥è¯†æ—¶ï¼‰"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®äº†ä»¥ä¸‹é—®é¢˜ï¼Œä½†æˆ‘åœ¨åŒ»å­¦çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„ä¿¡æ¯æ¥æä¾›å‡†ç¡®çš„åŒ»ç–—å»ºè®®ã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

        ã€é—®é¢˜ç†è§£ã€‘
        - ç®€è¦ç†è§£ç”¨æˆ·çš„é—®é¢˜å’Œå…³æ³¨ç‚¹

        ã€ä¸€èˆ¬æ€§å»ºè®®ã€‘
        - æä¾›ä¸€èˆ¬æ€§çš„å¥åº·å»ºè®®å’Œæ³¨æ„äº‹é¡¹
        - å»ºè®®ç”¨æˆ·å…³æ³¨çš„ç›¸å…³ç—‡çŠ¶æˆ–ä½“å¾
        - ç»™å‡ºåŸºæœ¬çš„ç”Ÿæ´»è°ƒç†å»ºè®®

        ã€å°±åŒ»å»ºè®®ã€‘
        - å¼ºçƒˆå»ºè®®ç”¨æˆ·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
        - å»ºè®®åˆé€‚çš„å°±è¯Šç§‘å®¤
        - æé†’åŠæ—¶å°±åŒ»çš„é‡è¦æ€§

        ã€é‡è¦æé†’ã€‘
        - å¼ºè°ƒæ­¤å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
        - æé†’ç”¨æˆ·åŠæ—¶å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®ä¿¡æ¯
        - å¦‚æœ‰ç´§æ€¥æƒ…å†µè¯·ç«‹å³å°±åŒ»

        è¦æ±‚ï¼š
        - è¯­è¨€æ¸©å’Œã€ä¸“ä¸š
        - é¿å…ç»™å‡ºå…·ä½“çš„è¯Šæ–­æˆ–æ²»ç–—å»ºè®®
        - é‡ç‚¹å¼ºè°ƒå’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿçš„é‡è¦æ€§
        - ä¿æŒå›ç­”çš„è°¨æ…æ€§å’Œå®‰å…¨æ€§"""
        
        return prompt
    
    def summarize_text(self, text: str, max_length: int = 200) -> str:
        """
        æ–‡æœ¬æ‘˜è¦
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        try:
            prompt = f"è¯·ä¸ºä»¥ä¸‹åŒ»å­¦æ–‡æœ¬æä¾›ç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡{max_length}å­—ï¼‰ï¼š\n\n{text}"
            
            summary = self.generate_text(
                prompt,
                max_new_tokens=max_length,
                temperature=0.3
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Error summarizing text: {e}")
            return text[:max_length] + "..." if len(text) > max_length else text
    
    def extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            prompt = f"è¯·ä»ä»¥ä¸‹åŒ»å­¦æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š\n\n{text}"
            
            keywords_text = self.generate_text(
                prompt,
                max_new_tokens=100,
                temperature=0.3
            )
            
            # è§£æå…³é”®è¯
            keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            
            return keywords[:10]  # æœ€å¤šè¿”å›10ä¸ªå…³é”®è¯
            
        except Exception as e:
            logger.error(f"Error extracting keywords: {e}")
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        try:
            return {
                'model_path': self.model_path,
                'device': self.device,
                'max_length': self.max_length,
                'temperature': self.temperature,
                'top_p': self.top_p,
                'top_k': self.top_k,
                'repetition_penalty': self.repetition_penalty,
                'vocab_size': len(self.tokenizer) if self.tokenizer else 0,
                'model_loaded': self.model is not None
            }
        except Exception as e:
            logger.error(f"Error getting model info: {e}")
            return {}

    def generate_medical_response_stream(self, query: str, context: str = "",
                                       response_type: str = "general"):
        """
        æµå¼ç”ŸæˆåŒ»å­¦å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            response_type: å“åº”ç±»å‹
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            logger.info("=" * 40)
            logger.info("ğŸŒŠ å¼€å§‹LLMæµå¼åŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹")
            logger.info("=" * 40)
            logger.info(f"è¾“å…¥å‚æ•°: query='{query}', contexté•¿åº¦={len(context)}, response_type='{response_type}'")
            
            # æ„å»ºåŒ»å­¦å›ç­”æç¤ºè¯
            logger.info("æ­¥éª¤1: å¼€å§‹æ„å»ºæµå¼åŒ»å­¦å›ç­”æç¤ºè¯")
            logger.info(f"ç”¨æˆ·æŸ¥è¯¢: '{query}'")
            logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            logger.info(f"å“åº”ç±»å‹: {response_type}")
            
            if context:
                logger.info("ä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯æ¨¡æ¿")
                prompt = f"""åŸºäºä»¥ä¸‹åŒ»å­¦çŸ¥è¯†å›ç­”é—®é¢˜ï¼š

åŒ»å­¦çŸ¥è¯†ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
                logger.info(f"å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹: {context}")
            else:
                logger.info("ä½¿ç”¨æ— ä¸Šä¸‹æ–‡çš„æç¤ºè¯æ¨¡æ¿")
                prompt = f"""è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ï¼š

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
            
            logger.info(f"æç¤ºè¯æ„å»ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"å®Œæ•´æç¤ºè¯å†…å®¹: {prompt}")
            logger.info("æ­¥éª¤1: æ„å»ºæµå¼åŒ»å­¦å›ç­”æç¤ºè¯å®Œæˆ")
            
            # æµå¼ç”Ÿæˆ
            logger.info("æ­¥éª¤2: å¼€å§‹æµå¼ç”ŸæˆåŒ»å­¦å›ç­”")
            logger.info("è°ƒç”¨å†…éƒ¨æµå¼ç”Ÿæˆæ–¹æ³•: _generate_stream")
            
            chunk_count = 0
            total_chars = 0
            
            for chunk in self._generate_stream(prompt):
                chunk_count += 1
                total_chars += len(chunk)
                logger.info(f"ç”Ÿæˆç¬¬{chunk_count}ä¸ªæ–‡æœ¬å—ï¼Œé•¿åº¦: {len(chunk)} å­—ç¬¦ï¼Œå†…å®¹: '{chunk}'")
                yield chunk
            
            logger.info(f"æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {chunk_count} ä¸ªæ–‡æœ¬å—ï¼Œæ€»é•¿åº¦: {total_chars} å­—ç¬¦")
            logger.info("æ­¥éª¤2: æµå¼ç”ŸæˆåŒ»å­¦å›ç­”å®Œæˆ")
            logger.info("=" * 40)
            logger.info("ğŸŒŠ LLMæµå¼åŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹å®Œæˆ")
            logger.info("=" * 40)
                
        except Exception as e:
            logger.error(f"æµå¼ç”ŸæˆåŒ»å­¦å›ç­”å‡ºé”™ï¼š{e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.info("=" * 40)
            logger.info("ğŸŒŠ LLMæµå¼åŒ»ç–—å“åº”ç”Ÿæˆæµç¨‹å¤±è´¥")
            logger.info("=" * 40)
            yield f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"

    
    def _generate_stream(self, prompt: str, max_new_tokens: int = 256):
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            if not self.tokenizer or not self.model:
                yield "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®"
                return
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå‚æ•°
            generation_config = {
                'max_new_tokens': max_new_tokens,
                'temperature': self.temperature,
                'top_p': self.top_p,
                'top_k': self.top_k,
                'repetition_penalty': self.repetition_penalty,
                'do_sample': True,
                'pad_token_id': self.tokenizer.eos_token_id
            }
            
            # æµå¼ç”Ÿæˆ
            with torch.no_grad():
                for output in self.model.generate(
                    inputs,
                    **generation_config,
                    return_dict_in_generate=True,
                    output_scores=True
                ):
                    # æ£€æŸ¥outputç±»å‹å¹¶è§£ç æ–°ç”Ÿæˆçš„token
                    if hasattr(output, 'sequences') and output.sequences is not None:
                        new_tokens = output.sequences[0][inputs.shape[1]:]
                        if len(new_tokens) > 0:
                            new_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
                            if new_text:
                                yield new_text
                    else:
                        # å¦‚æœoutputä¸æ˜¯é¢„æœŸçš„æ ¼å¼ï¼Œè·³è¿‡
                        continue
                            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆæ–‡æœ¬å‡ºé”™ï¼š{e}")
            yield f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™ï¼š{str(e)}"


class LLMServiceFactory:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å·¥å‚ç±»"""
    
    @staticmethod
    def create_llm_service(config_path: str = None) -> LLMService:
        """
        åˆ›å»ºå¤§è¯­è¨€æ¨¡å‹æœåŠ¡å®ä¾‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å®ä¾‹
        """
        # é»˜è®¤é…ç½®
        default_config = {
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'model_path': '/Users/tiangels/AI/llm_learning_project/zhi_zhen_tong_system/codes/ai_models/llm_models/Qwen2-0.5B-Medical-MLX',
            'max_length': 2048,
            'temperature': 0.7,
            'top_p': 0.9,
            'top_k': 50,
            'repetition_penalty': 1.1,
            'timeout': 600
        }
        
        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½é…ç½®
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    full_config = json.load(f)
                # æå–llm_serviceéƒ¨åˆ†çš„é…ç½®
                if 'llm_service' in full_config:
                    user_config = full_config['llm_service']
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"Error loading config file: {e}, using default config")
        
        return LLMService(default_config)


if __name__ == "__main__":
    # æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
    config = {
        'device': 'cpu',  # ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
        'model_path': 'ai_models/llm_models/Qwen2-0.5B-Medical-MLX',
        'max_length': 1024,
        'temperature': 0.7
    }
    
    try:
        llm_service = LLMService(config)
        
        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        test_prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯å¿ƒè‚Œæ¢—æ­»ï¼Ÿ"
        response = llm_service.generate_text(test_prompt, max_new_tokens=200)
        print(f"Generated response: {response}")
        
        # æµ‹è¯•åŒ»ç–—å“åº”ç”Ÿæˆ
        query = "æˆ‘æœ€è¿‘ç»å¸¸æ„Ÿåˆ°èƒ¸ç—›ï¼Œè¿™æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ"
        context = "èƒ¸ç—›å¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼ŒåŒ…æ‹¬å¿ƒè¡€ç®¡ç–¾ç—…ã€è‚ºéƒ¨ç–¾ç—…ç­‰ã€‚"
        
        medical_response = llm_service.generate_medical_response(
            query, context, response_type="diagnosis"
        )
        print(f"\nMedical response: {medical_response}")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = llm_service.get_model_info()
        print(f"\nModel info: {model_info}")
        
    except Exception as e:
        print(f"Error in testing: {e}")
        print("Please ensure the model is properly downloaded and configured.")
