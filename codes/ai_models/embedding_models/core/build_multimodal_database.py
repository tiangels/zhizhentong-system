"""
ç»Ÿä¸€çš„å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå™¨
æ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œå›¾æ–‡é…å¯¹æ•°æ®çš„å¤„ç†åŠŸèƒ½
æ”¯æŒæ™ºè¯Šé€šç³»ç»Ÿçš„å¤šæ¨¡æ€æ£€ç´¢éœ€æ±‚
"""

import os
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import logging
from tqdm import tqdm
import sys

# å¯¼å…¥æ–‡æ¡£åˆ‡åˆ†æ¨¡å—
from processors.document_chunker import DocumentChunker, ChunkConfig, ChunkStrategy, create_medical_chunker, create_general_chunker

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®æ–‡æ¡£åˆ‡åˆ†å™¨çš„æ—¥å¿—çº§åˆ«ä¸ºDEBUGï¼Œä»¥ä¾¿çœ‹åˆ°è¯¦ç»†çš„åˆ‡åˆ†è¿‡ç¨‹
document_chunker_logger = logging.getLogger('processors.document_chunker')
document_chunker_logger.setLevel(logging.INFO)

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å°è¯•å¯¼å…¥å›¾åƒå‘é‡åŒ–æ¨¡å—
try:
    # æ·»åŠ modelsç›®å½•åˆ°è·¯å¾„
    models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "models")
    sys.path.append(models_dir)
    from image_embedder import ImageEmbedderFactory, batch_embed_images
    IMAGE_EMBEDDING_AVAILABLE = True
    logger.info("âœ“ å›¾åƒå‘é‡åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.warning(f"å›¾åƒå‘é‡åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼: {e}")
    IMAGE_EMBEDDING_AVAILABLE = False

class UnifiedMultimodalVectorDatabaseBuilder:
    """ç»Ÿä¸€çš„å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå™¨"""
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–ç»Ÿä¸€çš„å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.text_embedder = None
        self.image_embedder = None
        self.text_vector_db = None
        self.image_vector_db = None
        self.multimodal_vector_db = None
        self.image_text_mapping = {}
        self.document_chunker = None  # æ–‡æ¡£åˆ‡åˆ†å™¨
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _load_config(self, config_path: str = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), "config.json")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        base_data_dir = "/Users/tiangels/AI/llm_learning_project/zhi_zhen_tong_system/datas/medical_knowledge"
        
        return {
            # æ•°æ®è·¯å¾„
            "BASE_DATA_DIR": base_data_dir,
            "PROCESSED_REPORTS_PATH": os.path.join(base_data_dir, "image_text_data", "processed", "processed_reports.csv"),
            "TRAIN_REPORTS_PATH": os.path.join(base_data_dir, "training_data", "dialogue_train.csv"),
            "TEST_REPORTS_PATH": os.path.join(base_data_dir, "test_data", "dialogue_test.csv"),
            "PROCESSED_IMAGES_PATH": os.path.join(base_data_dir, "image_text_data", "processed", "processed_images.npy"),
            "TRAIN_IMAGES_PATH": os.path.join(base_data_dir, "image_text_data", "processed", "train_images.npy"),
            "TEST_IMAGES_PATH": os.path.join(base_data_dir, "image_text_data", "processed", "test_images.npy"),
            
            # å‘é‡æ•°æ®åº“è·¯å¾„
            "TEXT_VECTOR_DB_PATH": os.path.join(project_root, "datas", "vector_databases", "text"),
            "IMAGE_VECTOR_DB_PATH": os.path.join(project_root, "datas", "vector_databases", "image"),
            "MULTIMODAL_VECTOR_DB_PATH": os.path.join(project_root, "datas", "vector_databases", "multimodal"),
            
            # æ¨¡å‹é…ç½®
            "TEXT_EMBEDDING_MODEL": "shibing624/text2vec-base-chinese",
            "IMAGE_EMBEDDING_MODEL": "resnet50",
            "IMAGE_EMBEDDER_TYPE": "resnet",
            "IMAGE_EMBEDDER_DEVICE": "cpu",
            
            # å¤„ç†é…ç½®
            "BATCH_SIZE": 100,
            "IMAGE_BATCH_SIZE": 32,
            "MAX_IMAGES_PER_BATCH": 500,
            "DOWNLOAD_TIMEOUT": 30,
            "MAX_RETRIES": 3,
            
            # é›†åˆåç§°
            "TEXT_COLLECTION_NAME": "medical_text_vectors",
            "IMAGE_COLLECTION_NAME": "medical_image_vectors",
            "MULTIMODAL_COLLECTION_NAME": "medical_multimodal_vectors",
            
            # å›¾åƒå‘é‡åŒ–é…ç½®
            "IMAGE_EMBEDDING_ENABLED": True,
            "PROXY": None,
            "LOCAL_MODEL_PATH": None
        }
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            self._set_environment_variables()
            
            # åˆå§‹åŒ–æ–‡æ¡£åˆ‡åˆ†å™¨
            self._init_document_chunker()
            
            # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
            self._init_text_embedder()
            
            # åˆå§‹åŒ–å›¾åƒåµŒå…¥æ¨¡å‹
            self._init_image_embedder()
            
            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            self._init_vector_databases()
            
            logger.info("ç»Ÿä¸€å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ç»„ä»¶å¤±è´¥: {e}")
            raise
    
    def _init_document_chunker(self):
        """åˆå§‹åŒ–æ–‡æ¡£åˆ‡åˆ†å™¨"""
        try:
            # åˆ›å»ºé€šç”¨æ–‡æ¡£åˆ‡åˆ†å™¨ï¼ˆæ›´é€‚åˆå¤„ç†å„ç§ç±»å‹çš„æ–‡æœ¬ï¼‰
            self.document_chunker = create_general_chunker()
            logger.info("æ–‡æ¡£åˆ‡åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ‡åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤åˆ‡åˆ†å™¨ä½œä¸ºå¤‡ç”¨
            self.document_chunker = DocumentChunker()
            logger.warning("ä½¿ç”¨é»˜è®¤æ–‡æ¡£åˆ‡åˆ†å™¨")
    
    def _set_environment_variables(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬ä»£ç†"""
        if self.config.get("PROXY"):
            os.environ["HTTP_PROXY"] = self.config["PROXY"]
            os.environ["HTTPS_PROXY"] = self.config["PROXY"]
            logger.info(f"å·²è®¾ç½®ä»£ç†: {self.config['PROXY']}")
        else:
            # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„ä»£ç†è®¾ç½®
            if "HTTP_PROXY" in os.environ:
                del os.environ["HTTP_PROXY"]
            if "HTTPS_PROXY" in os.environ:
                del os.environ["HTTPS_PROXY"]
        
        # è®¾ç½®ä¸‹è½½è¶…æ—¶
        if self.config.get("DOWNLOAD_TIMEOUT"):
            os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = str(self.config["DOWNLOAD_TIMEOUT"])
            logger.info(f"å·²è®¾ç½®HF_HUB_DOWNLOAD_TIMEOUT: {self.config['DOWNLOAD_TIMEOUT']}ç§’")
    
    def _init_text_embedder(self):
        """åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹"""
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
            
            # åŠ è½½åµŒå…¥æ¨¡å‹çš„å‚æ•°
            model_kwargs = {"device": "cpu"}
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            if self.config.get("LOCAL_MODEL_PATH"):
                logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {self.config['LOCAL_MODEL_PATH']}")
                self.text_embedder = HuggingFaceEmbeddings(
                    model_name=self.config["LOCAL_MODEL_PATH"],
                    model_kwargs=model_kwargs
                )
            else:
                # åœ¨çº¿ä¸‹è½½æ¨¡å‹ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
                from tenacity import retry, stop_after_attempt, wait_exponential
                
                @retry(stop=stop_after_attempt(self.config["MAX_RETRIES"]), 
                       wait=wait_exponential(multiplier=1, min=1, max=10))
                def load_embeddings_with_retry():
                    from langchain_huggingface import HuggingFaceEmbeddings
                    return HuggingFaceEmbeddings(
                        model_name=self.config["TEXT_EMBEDDING_MODEL"],
                        model_kwargs=model_kwargs
                    )
                
                logger.info(f"å°è¯•ä¸‹è½½æ¨¡å‹ï¼Œæœ€å¤šé‡è¯• {self.config['MAX_RETRIES']} æ¬¡...")
                self.text_embedder = load_embeddings_with_retry()
            
            logger.info(f"æ–‡æœ¬åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.config['TEXT_EMBEDDING_MODEL']}")
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_image_embedder(self):
        """åˆå§‹åŒ–å›¾åƒåµŒå…¥æ¨¡å‹"""
        if not self.config.get("IMAGE_EMBEDDING_ENABLED", False) or not IMAGE_EMBEDDING_AVAILABLE:
            logger.info("å›¾åƒå‘é‡åŒ–åŠŸèƒ½å·²ç¦ç”¨æˆ–æ¨¡å—ä¸å¯ç”¨")
            return
        
        try:
            logger.info(f"åˆå§‹åŒ–å›¾åƒå‘é‡åŒ–å™¨: {self.config['IMAGE_EMBEDDER_TYPE']} ({self.config['IMAGE_EMBEDDING_MODEL']})")
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.config['IMAGE_EMBEDDER_DEVICE']}")
            
            # åˆ›å»ºå›¾åƒå‘é‡åŒ–å™¨
            self.image_embedder = ImageEmbedderFactory.create_embedder(
                embedder_type=self.config["IMAGE_EMBEDDER_TYPE"],
                model_name=self.config["IMAGE_EMBEDDING_MODEL"],
                device=self.config["IMAGE_EMBEDDER_DEVICE"]
            )
            
            logger.info(f"å›¾åƒå‘é‡åŒ–å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹ç±»å‹: {self.config['IMAGE_EMBEDDER_TYPE']}")
            
        except Exception as e:
            logger.error(f"å›¾åƒå‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.image_embedder = None
    
    def _init_vector_databases(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            from langchain_chroma import Chroma
            
            # åˆ›å»ºå‘é‡æ•°æ®åº“ç›®å½•
            os.makedirs(self.config["TEXT_VECTOR_DB_PATH"], exist_ok=True)
            os.makedirs(self.config["IMAGE_VECTOR_DB_PATH"], exist_ok=True)
            os.makedirs(self.config["MULTIMODAL_VECTOR_DB_PATH"], exist_ok=True)
            
            # åˆå§‹åŒ–æ–‡æœ¬å‘é‡æ•°æ®åº“ï¼ˆå·²æ³¨é‡Š - å†—ä½™åŠŸèƒ½ï¼‰
            # self.text_vector_db = Chroma(
            #     persist_directory=self.config["TEXT_VECTOR_DB_PATH"],
            #     embedding_function=self.text_embedder,
            #     collection_name=self.config["TEXT_COLLECTION_NAME"]
            # )
            
            # åˆå§‹åŒ–å›¾åƒå‘é‡æ•°æ®åº“ï¼ˆå·²æ³¨é‡Š - å†—ä½™åŠŸèƒ½ï¼‰
            # if self.image_embedder:
            #     # ä¸ºå›¾åƒå‘é‡åˆ›å»ºä¸€ä¸ªç®€å•çš„åµŒå…¥å‡½æ•°
            #     class DummyEmbedding:
            #         def embed_documents(self, texts):
            #             return [[0.0] * 2048] * len(texts)  # ResNet50è¾“å‡º2048ç»´
            #         
            #         def embed_query(self, text):
            #             return [0.0] * 2048
            #     
            #     self.image_vector_db = Chroma(
            #         collection_name=self.config["IMAGE_COLLECTION_NAME"],
            #         embedding_function=DummyEmbedding(),
            #         persist_directory=self.config["IMAGE_VECTOR_DB_PATH"]
            #     )
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€å‘é‡æ•°æ®åº“
            self.multimodal_vector_db = Chroma(
                persist_directory=self.config["MULTIMODAL_VECTOR_DB_PATH"],
                embedding_function=self.text_embedder,
                collection_name=self.config["MULTIMODAL_COLLECTION_NAME"]
            )
            
            logger.info("å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def load_data(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®"""
        try:
            print("=" * 50)
            print("ğŸ“ æ–‡æ¡£åŠ è½½å¼€å§‹")
            print("=" * 50)
            logger.info("å¼€å§‹åŠ è½½å¤„ç†åçš„æ•°æ®...")
            
            # åŠ è½½å›¾æ–‡æ•°æ®ï¼ˆåŸæœ‰çš„image_text_dataï¼‰
            logger.info("æ­£åœ¨åŠ è½½å›¾æ–‡æ•°æ®...")
            processed_reports = pd.read_csv(self.config["PROCESSED_REPORTS_PATH"]) if os.path.exists(self.config["PROCESSED_REPORTS_PATH"]) else pd.DataFrame()
            train_reports = pd.read_csv(self.config["TRAIN_REPORTS_PATH"]) if os.path.exists(self.config["TRAIN_REPORTS_PATH"]) else pd.DataFrame()
            test_reports = pd.read_csv(self.config["TEST_REPORTS_PATH"]) if os.path.exists(self.config["TEST_REPORTS_PATH"]) else pd.DataFrame()
            
            logger.info(f"å›¾æ–‡æ•°æ®åŠ è½½å®Œæˆ:")
            logger.info(f"  - å¤„ç†åçš„æŠ¥å‘Š: {len(processed_reports)} æ¡")
            logger.info(f"  - è®­ç»ƒé›†æŠ¥å‘Š: {len(train_reports)} æ¡")
            logger.info(f"  - æµ‹è¯•é›†æŠ¥å‘Š: {len(test_reports)} æ¡")
            
            # åŠ è½½å›¾åƒæ•°æ®
            logger.info("æ­£åœ¨åŠ è½½å›¾åƒæ•°æ®...")
            processed_images = np.load(self.config["PROCESSED_IMAGES_PATH"], allow_pickle=True) if os.path.exists(self.config["PROCESSED_IMAGES_PATH"]) else np.array([])
            train_images = np.load(self.config["TRAIN_IMAGES_PATH"], allow_pickle=True) if os.path.exists(self.config["TRAIN_IMAGES_PATH"]) else np.array([])
            test_images = np.load(self.config["TEST_IMAGES_PATH"], allow_pickle=True) if os.path.exists(self.config["TEST_IMAGES_PATH"]) else np.array([])
            
            logger.info(f"å›¾åƒæ•°æ®åŠ è½½å®Œæˆ:")
            logger.info(f"  - å¤„ç†åçš„å›¾åƒ: {len(processed_images)} å¼ ")
            logger.info(f"  - è®­ç»ƒé›†å›¾åƒ: {len(train_images)} å¼ ")
            logger.info(f"  - æµ‹è¯•é›†å›¾åƒ: {len(test_images)} å¼ ")
            
            # åŠ è½½çº¯æ–‡æœ¬æ•°æ®ï¼ˆtext_dataï¼‰
            logger.info("æ­£åœ¨åŠ è½½çº¯æ–‡æœ¬æ•°æ®...")
            text_data_base = "/Users/tiangels/AI/llm_learning_project/zhi_zhen_tong_system/datas/medical_knowledge/text_data/processed"
            general_text_train = pd.read_csv(f"{text_data_base}/training_data/general_text_train.csv") if os.path.exists(f"{text_data_base}/training_data/general_text_train.csv") else pd.DataFrame()
            general_text_test = pd.read_csv(f"{text_data_base}/test_data/general_text_test.csv") if os.path.exists(f"{text_data_base}/test_data/general_text_test.csv") else pd.DataFrame()
            
            logger.info(f"çº¯æ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆ:")
            logger.info(f"  - çº¯æ–‡æœ¬è®­ç»ƒæ•°æ®: {len(general_text_train)} æ¡")
            logger.info(f"  - çº¯æ–‡æœ¬æµ‹è¯•æ•°æ®: {len(general_text_test)} æ¡")
            
            # æ•°æ®åŠ è½½ç»Ÿè®¡
            total_documents = len(processed_reports) + len(train_reports) + len(test_reports) + len(general_text_train) + len(general_text_test)
            total_images = len(processed_images) + len(train_images) + len(test_images)
            
            print("âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"  - æ€»æ–‡æ¡£æ•°: {total_documents} æ¡")
            print(f"  - æ€»å›¾åƒæ•°: {total_images} å¼ ")
            print("=" * 50)
            print("ğŸ“ æ–‡æ¡£åŠ è½½ç»“æŸ")
            print("=" * 50)
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®¡: {total_documents} ä¸ªæ–‡æ¡£, {total_images} å¼ å›¾åƒ")
            
            return {
                "processed_reports": processed_reports,
                "train_reports": train_reports,
                "test_reports": test_reports,
                "processed_images": processed_images,
                "train_images": train_images,
                "test_images": test_images,
                "general_text_train": general_text_train,
                "general_text_test": general_text_test
            }
            
        except Exception as e:
            print("âŒ æ–‡æ¡£åŠ è½½å¤±è´¥")
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    def convert_to_basic_type(self, value):
        """å°†å€¼è½¬æ¢ä¸ºChromaDBæ”¯æŒçš„åŸºæœ¬ç±»å‹"""
        if pd.isna(value):
            return None
        elif isinstance(value, (str, int, float, bool)):
            return value
        elif isinstance(value, (list, tuple)):
            return str(value)
        elif isinstance(value, dict):
            return str(value)
        elif hasattr(value, 'shape'):
            shape = value.shape
            if len(shape) == 3:
                return f"{shape[0]}x{shape[1]}x{shape[2]}"
            elif len(shape) == 2:
                return f"{shape[0]}x{shape[1]}"
            else:
                return str(shape)
        else:
            return str(value)
    
    def prepare_general_text_documents(self, text_df: pd.DataFrame, dataset_type: str = "general_text") -> Tuple[List[str], List[Dict]]:
        """å‡†å¤‡çº¯æ–‡æœ¬æ–‡æ¡£æ•°æ®ç”¨äºå‘é‡å­˜å‚¨ï¼ˆåŒ…å«æ–‡æ¡£åˆ‡åˆ†ï¼‰"""
        documents = []
        metadatas = []
        
        print("=" * 50)
        print("ğŸ§¹ æ•°æ®æ¸…æ´—å¼€å§‹")
        print("=" * 50)
        logger.info(f"å¼€å§‹å‡†å¤‡ {dataset_type} æ•°æ®é›†çš„çº¯æ–‡æœ¬æ–‡æ¡£...")
        
        # æ•°æ®æ¸…æ´—ç»Ÿè®¡
        total_rows = len(text_df)
        valid_rows = 0
        invalid_rows = 0
        empty_content_rows = 0
        short_content_rows = 0
        
        logger.info(f"å¼€å§‹æ•°æ®æ¸…æ´—ï¼Œæ€»è¡Œæ•°: {total_rows}")
        
        for idx, row in tqdm(text_df.iterrows(), total=len(text_df)):
            # è·å–æ–‡æ¡£å†…å®¹
            content = row.get('content', '')
            
            # æ•°æ®æ¸…æ´—æ­¥éª¤
            if not content:
                empty_content_rows += 1
                invalid_rows += 1
                logger.debug(f"è¡Œ {idx}: å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            if len(content.strip()) < 10:
                short_content_rows += 1
                invalid_rows += 1
                logger.debug(f"è¡Œ {idx}: å†…å®¹è¿‡çŸ­ ({len(content.strip())} å­—ç¬¦)ï¼Œè·³è¿‡")
                continue
            
            valid_rows += 1
            
            # ä½¿ç”¨æ–‡æ¡£åˆ‡åˆ†å™¨åˆ‡åˆ†æ–‡æ¡£
            if self.document_chunker:
                # åˆ›å»ºåŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    "original_document_id": f"{dataset_type}_doc_{idx}",
                    "dataset_type": dataset_type,
                    "original_index": idx,
                    "content_type": "general_text",
                    "source_file": row.get('source_file', ''),
                    "file_type": row.get('file_type', ''),
                    "original_id": row.get('id', idx)
                }
                
                # æ·»åŠ å…¶ä»–å…ƒæ•°æ®å­—æ®µ
                if 'metadata' in row and pd.notna(row['metadata']):
                    if isinstance(row['metadata'], dict):
                        base_metadata.update(row['metadata'])
                    else:
                        base_metadata['original_metadata'] = str(row['metadata'])
                
                # æ·»åŠ åˆ†è¯ç»“æœ
                if 'content_tokens' in row and pd.notna(row['content_tokens']):
                    base_metadata['content_tokens'] = row['content_tokens']
                
                print("=" * 50)
                print("âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†å¼€å§‹")
                print("=" * 50)
                logger.info(f"æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£ {idx}: {content[:100]}...")
                
                # åˆ‡åˆ†æ–‡æ¡£
                chunks = self.document_chunker.chunk_document(content, base_metadata)
                
                print("âœ… æ–‡æ¡£åˆ‡åˆ†å®Œæˆ")
                print(f"ğŸ“Š åˆ‡åˆ†ç»Ÿè®¡:")
                print(f"  - æ–‡æ¡£ID: {idx}")
                print(f"  - åŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"  - åˆ‡åˆ†ç‰‡æ®µæ•°: {len(chunks)}")
                print("=" * 50)
                print("âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†ç»“æŸ")
                print("=" * 50)
                
                logger.info(f"æ–‡æ¡£ {idx} åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
                
                # å°†åˆ‡åˆ†åçš„æ–‡æ¡£æ·»åŠ åˆ°ç»“æœä¸­
                for chunk in chunks:
                    documents.append(chunk['content'])
                    metadatas.append(chunk['metadata'])
            else:
                # å¦‚æœæ²¡æœ‰æ–‡æ¡£åˆ‡åˆ†å™¨ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                logger.warning(f"æ–‡æ¡£åˆ‡åˆ†å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {idx}")
                
                # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œè¿›è¡Œç®€å•æˆªæ–­
                if len(content) > 1000:
                    content = content[:1000] + "..."
                
                # ç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºå•ä¸ªæ–‡æ¡£
                chunks = [content] if len(content.strip()) > 10 else []
                
                # ä¸ºæ¯ä¸ªæ–‡æ¡£å—åˆ›å»ºå‘é‡å­˜å‚¨æ¡ç›®
                for chunk_idx, chunk in enumerate(chunks):
                    if len(chunk.strip()) < 10:  # è¿‡æ»¤å¤ªçŸ­çš„å—
                        continue
                    
                    # åˆ›å»ºæ–‡æ¡£ID
                    doc_id = f"{dataset_type}_{row.get('id', idx)}_{chunk_idx}"
                    
                    # åˆ›å»ºå…ƒæ•°æ®
                    metadata = {
                        'doc_id': doc_id,
                        'dataset_type': dataset_type,
                        'source_file': row.get('source_file', ''),
                        'file_type': row.get('file_type', ''),
                        'chunk_index': chunk_idx,
                        'total_chunks': len(chunks),
                        'original_id': row.get('id', idx)
                    }
                    
                    # æ·»åŠ å…¶ä»–å…ƒæ•°æ®å­—æ®µ
                    if 'metadata' in row and pd.notna(row['metadata']):
                        if isinstance(row['metadata'], dict):
                            metadata.update(row['metadata'])
                        else:
                            metadata['original_metadata'] = str(row['metadata'])
                    
                    # æ·»åŠ åˆ†è¯ç»“æœ
                    if 'content_tokens' in row and pd.notna(row['content_tokens']):
                        metadata['content_tokens'] = row['content_tokens']
                    
                    # è½¬æ¢å…ƒæ•°æ®ä¸ºåŸºæœ¬ç±»å‹
                    metadata = {k: self.convert_to_basic_type(v) for k, v in metadata.items()}
                    
                    documents.append(chunk)
                    metadatas.append(metadata)
        
        # æ•°æ®æ¸…æ´—ç»“æœç»Ÿè®¡
        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡:")
        print(f"  - æ€»è¡Œæ•°: {total_rows}")
        print(f"  - æœ‰æ•ˆè¡Œæ•°: {valid_rows}")
        print(f"  - æ— æ•ˆè¡Œæ•°: {invalid_rows}")
        print(f"  - ç©ºå†…å®¹è¡Œæ•°: {empty_content_rows}")
        print(f"  - çŸ­å†…å®¹è¡Œæ•°: {short_content_rows}")
        print(f"  - æ¸…æ´—åæ–‡æ¡£å—æ•°: {len(documents)}")
        print("=" * 50)
        print("ğŸ§¹ æ•°æ®æ¸…æ´—ç»“æŸ")
        print("=" * 50)
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ:")
        logger.info(f"  - æ€»è¡Œæ•°: {total_rows}")
        logger.info(f"  - æœ‰æ•ˆè¡Œæ•°: {valid_rows}")
        logger.info(f"  - æ— æ•ˆè¡Œæ•°: {invalid_rows}")
        logger.info(f"  - ç©ºå†…å®¹è¡Œæ•°: {empty_content_rows}")
        logger.info(f"  - çŸ­å†…å®¹è¡Œæ•°: {short_content_rows}")
        logger.info(f"çº¯æ–‡æœ¬æ–‡æ¡£å‡†å¤‡å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
        return documents, metadatas

    def prepare_text_documents(self, reports_df: pd.DataFrame, images: np.ndarray = None, dataset_type: str = "processed") -> Tuple[List[str], List[Dict]]:
        """å‡†å¤‡æ–‡æœ¬æ–‡æ¡£æ•°æ®ç”¨äºå‘é‡å­˜å‚¨ï¼ˆåŒ…å«æ–‡æ¡£åˆ‡åˆ†ï¼‰"""
        documents = []
        metadatas = []
        
        print("=" * 50)
        print("ğŸ§¹ æ•°æ®æ¸…æ´—å¼€å§‹")
        print("=" * 50)
        logger.info(f"å¼€å§‹å‡†å¤‡ {dataset_type} æ•°æ®é›†çš„æ–‡æœ¬æ–‡æ¡£...")
        
        # æ•°æ®æ¸…æ´—ç»Ÿè®¡
        total_rows = len(reports_df)
        valid_rows = 0
        invalid_rows = 0
        empty_content_rows = 0
        
        logger.info(f"å¼€å§‹æ•°æ®æ¸…æ´—ï¼Œæ€»è¡Œæ•°: {total_rows}")
        
        for idx, row in tqdm(reports_df.iterrows(), total=len(reports_df)):
            # åˆ›å»ºæ–‡æ¡£å†…å®¹
            content_parts = []
            if 'description' in row and pd.notna(row['description']):
                content_parts.append(f"ç—…æƒ…æè¿°: {row['description']}")
            if 'diagnosis' in row and pd.notna(row['diagnosis']):
                content_parts.append(f"è¯Šæ–­ç»“æœ: {row['diagnosis']}")
            if 'suggestions' in row and pd.notna(row['suggestions']):
                content_parts.append(f"åŒ»ç”Ÿå»ºè®®: {row['suggestions']}")
            if 'dialogue_content' in row and pd.notna(row['dialogue_content']):
                content_parts.append(f"å¯¹è¯å†…å®¹: {row['dialogue_content']}")
            if 'findings' in row and pd.notna(row['findings']):
                content_parts.append(f"æ£€æŸ¥ç»“æœ: {row['findings']}")
            if 'impression' in row and pd.notna(row['impression']):
                content_parts.append(f"å°è±¡: {row['impression']}")
            
            # æ·»åŠ åˆ†è¯åçš„å†…å®¹
            if 'description_tokens' in row and pd.notna(row['description_tokens']):
                content_parts.append(f"ç—…æƒ…æè¿°åˆ†è¯: {row['description_tokens']}")
            if 'diagnosis_tokens' in row and pd.notna(row['diagnosis_tokens']):
                content_parts.append(f"è¯Šæ–­ç»“æœåˆ†è¯: {row['diagnosis_tokens']}")
            if 'suggestions_tokens' in row and pd.notna(row['suggestions_tokens']):
                content_parts.append(f"åŒ»ç”Ÿå»ºè®®åˆ†è¯: {row['suggestions_tokens']}")
            if 'dialogue_content_tokens' in row and pd.notna(row['dialogue_content_tokens']):
                content_parts.append(f"å¯¹è¯å†…å®¹åˆ†è¯: {row['dialogue_content_tokens']}")
            if 'findings_tokens' in row and pd.notna(row['findings_tokens']):
                content_parts.append(f"æ£€æŸ¥ç»“æœåˆ†è¯: {row['findings_tokens']}")
            if 'impression_tokens' in row and pd.notna(row['impression_tokens']):
                content_parts.append(f"å°è±¡åˆ†è¯: {row['impression_tokens']}")
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†…å®¹ï¼Œè·³è¿‡æ­¤è®°å½•
            if not content_parts:
                empty_content_rows += 1
                invalid_rows += 1
                logger.debug(f"è¡Œ {idx}: æ²¡æœ‰æœ‰æ•ˆå†…å®¹å­—æ®µï¼Œè·³è¿‡")
                continue
            
            content = "\n".join(content_parts)
            valid_rows += 1
            
            # ä½¿ç”¨æ–‡æ¡£åˆ‡åˆ†å™¨åˆ‡åˆ†æ–‡æ¡£
            if self.document_chunker:
                # åˆ›å»ºåŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    "original_document_id": f"{dataset_type}_doc_{idx}",
                    "dataset_type": dataset_type,
                    "original_index": idx,
                    "content_type": "text"
                }
                
                # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®å­—æ®µ
                for col in reports_df.columns:
                    if col not in ['description', 'diagnosis', 'suggestions', 'dialogue_content', 'findings', 'impression',
                                  'description_tokens', 'diagnosis_tokens', 'suggestions_tokens', 'dialogue_content_tokens',
                                  'findings_tokens', 'impression_tokens'] and pd.notna(row[col]):
                        base_metadata[col] = self.convert_to_basic_type(row[col])
                
                # å¦‚æœæœ‰å›¾åƒæ•°æ®ï¼Œæ·»åŠ å›¾åƒä¿¡æ¯
                if images is not None and idx < len(images):
                    base_metadata["has_image"] = True
                    base_metadata["image_shape"] = self.convert_to_basic_type(images[idx].shape)
                else:
                    base_metadata["has_image"] = False
                
                print("=" * 50)
                print("âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†å¼€å§‹")
                print("=" * 50)
                logger.info(f"æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£ {idx}: {content[:100]}...")
                
                # åˆ‡åˆ†æ–‡æ¡£
                chunks = self.document_chunker.chunk_document(content, base_metadata)
                
                print("âœ… æ–‡æ¡£åˆ‡åˆ†å®Œæˆ")
                print(f"ğŸ“Š åˆ‡åˆ†ç»Ÿè®¡:")
                print(f"  - æ–‡æ¡£ID: {idx}")
                print(f"  - åŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"  - åˆ‡åˆ†ç‰‡æ®µæ•°: {len(chunks)}")
                print("=" * 50)
                print("âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†ç»“æŸ")
                print("=" * 50)
                
                logger.info(f"æ–‡æ¡£ {idx} åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
                
                # å°†åˆ‡åˆ†åçš„æ–‡æ¡£æ·»åŠ åˆ°ç»“æœä¸­
                for chunk in chunks:
                    documents.append(chunk['content'])
                    metadatas.append(chunk['metadata'])
            else:
                # å¦‚æœæ²¡æœ‰æ–‡æ¡£åˆ‡åˆ†å™¨ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                document_id = f"{dataset_type}_doc_{idx}"
                
                # åˆ›å»ºå…ƒæ•°æ®
                metadata = {
                    "document_id": document_id,
                    "dataset_type": dataset_type,
                    "index": idx,
                    "content_type": "text"
                }
                
                # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®å­—æ®µ
                for col in reports_df.columns:
                    if col not in ['description', 'diagnosis', 'suggestions', 'dialogue_content', 'findings', 'impression',
                                  'description_tokens', 'diagnosis_tokens', 'suggestions_tokens', 'dialogue_content_tokens',
                                  'findings_tokens', 'impression_tokens'] and pd.notna(row[col]):
                        metadata[col] = self.convert_to_basic_type(row[col])
                
                # å¦‚æœæœ‰å›¾åƒæ•°æ®ï¼Œæ·»åŠ å›¾åƒä¿¡æ¯
                if images is not None and idx < len(images):
                    metadata["has_image"] = True
                    metadata["image_shape"] = self.convert_to_basic_type(images[idx].shape)
                else:
                    metadata["has_image"] = False
                
                documents.append(content)
                metadatas.append(metadata)
        
        # æ•°æ®æ¸…æ´—ç»“æœç»Ÿè®¡
        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡:")
        print(f"  - æ€»è¡Œæ•°: {total_rows}")
        print(f"  - æœ‰æ•ˆè¡Œæ•°: {valid_rows}")
        print(f"  - æ— æ•ˆè¡Œæ•°: {invalid_rows}")
        print(f"  - ç©ºå†…å®¹è¡Œæ•°: {empty_content_rows}")
        print(f"  - æ¸…æ´—åæ–‡æ¡£å—æ•°: {len(documents)}")
        print("=" * 50)
        print("ğŸ§¹ æ•°æ®æ¸…æ´—ç»“æŸ")
        print("=" * 50)
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ:")
        logger.info(f"  - æ€»è¡Œæ•°: {total_rows}")
        logger.info(f"  - æœ‰æ•ˆè¡Œæ•°: {valid_rows}")
        logger.info(f"  - æ— æ•ˆè¡Œæ•°: {invalid_rows}")
        logger.info(f"  - ç©ºå†…å®¹è¡Œæ•°: {empty_content_rows}")
        logger.info(f"æ–‡æœ¬æ–‡æ¡£å‡†å¤‡å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return documents, metadatas
    
    def vectorize_images(self, images: np.ndarray, dataset_type: str = "processed") -> Tuple[Optional[np.ndarray], Optional[List[Dict]]]:
        """å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºå‘é‡"""
        if self.image_embedder is None or len(images) == 0:
            return None, None
        
        print("=" * 50)
        print("ğŸ”¢ å‘é‡åŒ–å¼€å§‹")
        print("=" * 50)
        logger.info(f"å¼€å§‹å°† {len(images)} å¼ å›¾åƒè½¬æ¢ä¸ºå‘é‡...")
        
        try:
            # ä½¿ç”¨batch_embed_imageså‡½æ•°æ‰¹é‡å¤„ç†å›¾åƒ
            logger.info(f"ä½¿ç”¨æ‰¹é‡å¤„ç†ï¼Œæ‰¹æ¬¡å¤§å°: {self.config['IMAGE_BATCH_SIZE']}")
            embeddings = batch_embed_images(self.image_embedder, images, batch_size=self.config["IMAGE_BATCH_SIZE"])
            
            # åˆ›å»ºå…ƒæ•°æ®
            logger.info("æ­£åœ¨åˆ›å»ºå›¾åƒå‘é‡å…ƒæ•°æ®...")
            metadatas = []
            for idx in range(len(embeddings)):
                metadata = {
                    "document_id": f"{dataset_type}_image_{idx}",
                    "dataset_type": dataset_type,
                    "index": idx,
                    "content_type": "image",
                    "vector_dim": len(embeddings[idx])
                }
                metadatas.append(metadata)
            
            print("âœ… å‘é‡åŒ–å®Œæˆ")
            print(f"ğŸ“Š å‘é‡åŒ–ç»Ÿè®¡:")
            print(f"  - è¾“å…¥å›¾åƒæ•°: {len(images)}")
            print(f"  - è¾“å‡ºå‘é‡æ•°: {len(embeddings)}")
            print(f"  - å‘é‡ç»´åº¦: {len(embeddings[0]) if len(embeddings) > 0 else 0}")
            print(f"  - æ•°æ®é›†ç±»å‹: {dataset_type}")
            print("=" * 50)
            print("ğŸ”¢ å‘é‡åŒ–ç»“æŸ")
            print("=" * 50)
            
            logger.info(f"å›¾åƒå‘é‡åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(embeddings)} ä¸ªå›¾åƒå‘é‡")
            return embeddings, metadatas
            
        except Exception as e:
            print("âŒ å‘é‡åŒ–å¤±è´¥")
            logger.error(f"å›¾åƒå‘é‡åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def build_image_text_mapping(self, reports_df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """æ„å»ºå›¾åƒå’Œæ–‡æœ¬çš„æ˜ å°„å…³ç³»"""
        mapping = {}
        
        for idx, row in reports_df.iterrows():
            # å°è¯•è·å–uidæˆ–idå­—æ®µ
            uid = str(row.get('uid', row.get('id', idx)))
            
            # æ„å»ºæ–‡æœ¬å†…å®¹
            text_parts = []
            
            # æ£€æŸ¥å„ç§å¯èƒ½çš„å­—æ®µ
            if 'findings' in row and pd.notna(row['findings']):
                text_parts.append(f"æ£€æŸ¥ç»“æœ: {row['findings']}")
            if 'impression' in row and pd.notna(row['impression']):
                text_parts.append(f"å°è±¡: {row['impression']}")
            if 'indication' in row and pd.notna(row['indication']):
                text_parts.append(f"é€‚åº”ç—‡: {row['indication']}")
            if 'comparison' in row and pd.notna(row['comparison']):
                text_parts.append(f"å¯¹æ¯”: {row['comparison']}")
            if 'description' in row and pd.notna(row['description']):
                text_parts.append(f"ç—…æƒ…æè¿°: {row['description']}")
            if 'diagnosis' in row and pd.notna(row['diagnosis']):
                text_parts.append(f"è¯Šæ–­ç»“æœ: {row['diagnosis']}")
            if 'suggestions' in row and pd.notna(row['suggestions']):
                text_parts.append(f"åŒ»ç”Ÿå»ºè®®: {row['suggestions']}")
            if 'dialogue_content' in row and pd.notna(row['dialogue_content']):
                text_parts.append(f"å¯¹è¯å†…å®¹: {row['dialogue_content']}")
            
            if text_parts:
                text_content = "\n".join(text_parts)
                mapping[uid] = {
                    'text': text_content,
                    'index': idx,
                    'metadata': {
                        'uid': uid,
                        'MeSH': row.get('MeSH', ''),
                        'Problems': row.get('Problems', ''),
                        'image': row.get('image', ''),
                        'indication': row.get('indication', ''),
                        'comparison': row.get('comparison', ''),
                        'findings': row.get('findings', ''),
                        'impression': row.get('impression', ''),
                        'description': row.get('description', ''),
                        'diagnosis': row.get('diagnosis', ''),
                        'suggestions': row.get('suggestions', ''),
                        'dialogue_content': row.get('dialogue_content', '')
                    }
                }
        
        logger.info(f"å·²æ„å»º {len(mapping)} ä¸ªå›¾åƒæ–‡æœ¬æ˜ å°„å…³ç³»")
        return mapping
    
    def vectorize_multimodal_data(self, reports_df: pd.DataFrame, images: np.ndarray) -> Tuple[List[str], List[Dict], List[np.ndarray]]:
        """å‘é‡åŒ–å¤šæ¨¡æ€æ•°æ®"""
        documents = []
        metadatas = []
        embeddings = []
        
        print("=" * 50)
        print("ğŸ”¢ å‘é‡åŒ–å¼€å§‹")
        print("=" * 50)
        logger.info("å¼€å§‹å‘é‡åŒ–å¤šæ¨¡æ€æ•°æ®...")
        
        # æ„å»ºå›¾åƒæ–‡æœ¬æ˜ å°„
        logger.info("æ­£åœ¨æ„å»ºå›¾åƒæ–‡æœ¬æ˜ å°„å…³ç³»...")
        self.image_text_mapping = self.build_image_text_mapping(reports_df)
        
        # å‘é‡åŒ–ç»Ÿè®¡
        total_pairs = len(self.image_text_mapping)
        image_vectors = 0
        text_vectors = 0
        
        logger.info(f"å¼€å§‹å¤„ç† {total_pairs} ä¸ªå›¾æ–‡å¯¹...")
        
        # å¤„ç†æ¯ä¸ªå›¾æ–‡å¯¹
        for uid, mapping_info in tqdm(self.image_text_mapping.items(), desc="å‘é‡åŒ–å¤šæ¨¡æ€æ•°æ®"):
            idx = mapping_info['index']
            text_content = mapping_info['text']
            metadata = mapping_info['metadata'].copy()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å›¾åƒ
            if idx < len(images):
                # æœ‰å›¾åƒçš„æƒ…å†µï¼šä½¿ç”¨å›¾åƒå‘é‡
                image = images[idx]
                
                # ç”Ÿæˆå›¾åƒå‘é‡
                try:
                    import torch
                    from torchvision import transforms
                    
                    # é¢„å¤„ç†å›¾åƒ
                    if len(image.shape) == 3 and image.shape[0] == 3:  # CHWæ ¼å¼
                        image = np.transpose(image, (1, 2, 0))  # è½¬æ¢ä¸ºHWCæ ¼å¼
                    
                    # ç¡®ä¿å›¾åƒå€¼åœ¨0-255èŒƒå›´å†…ï¼ˆPIL ImageæœŸæœ›uint8ï¼‰
                    if image.max() <= 1.0:
                        image = (image * 255).astype(np.uint8)
                    else:
                        image = image.astype(np.uint8)
                    
                    # ç›´æ¥ä¼ é€’numpyæ•°ç»„ç»™embed_imageæ–¹æ³•
                    image_vector = self.image_embedder.embed_image(image)
                    
                    # ä½¿ç”¨å›¾åƒå‘é‡ä½œä¸ºåµŒå…¥
                    embeddings.append(image_vector)
                    image_vectors += 1
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    metadata.update({
                        'content_type': 'multimodal',
                        'has_image': True,
                        'image_index': idx,
                        'vector_type': 'image'
                    })
                    
                except Exception as e:
                    logger.warning(f"å›¾åƒå‘é‡åŒ–å¤±è´¥ (UID: {uid}): {e}")
                    # å›é€€åˆ°æ–‡æœ¬å‘é‡
                    text_vector = self.text_embedder.embed_query(text_content)
                    embeddings.append(text_vector)
                    text_vectors += 1
                    metadata.update({
                        'content_type': 'text',
                        'has_image': False,
                        'vector_type': 'text'
                    })
            else:
                # æ²¡æœ‰å›¾åƒçš„æƒ…å†µï¼šä½¿ç”¨æ–‡æœ¬å‘é‡
                text_vector = self.text_embedder.embed_query(text_content)
                embeddings.append(text_vector)
                text_vectors += 1
                metadata.update({
                    'content_type': 'text',
                    'has_image': False,
                    'vector_type': 'text'
                })
            
            # æ·»åŠ æ–‡æ¡£å’Œå…ƒæ•°æ®
            documents.append(text_content)
            metadatas.append(metadata)
        
        print("âœ… å‘é‡åŒ–å®Œæˆ")
        print(f"ğŸ“Š å‘é‡åŒ–ç»Ÿè®¡:")
        print(f"  - æ€»å›¾æ–‡å¯¹æ•°: {total_pairs}")
        print(f"  - å›¾åƒå‘é‡æ•°: {image_vectors}")
        print(f"  - æ–‡æœ¬å‘é‡æ•°: {text_vectors}")
        print(f"  - æ€»æ–‡æ¡£æ•°: {len(documents)}")
        print(f"  - æ€»å‘é‡æ•°: {len(embeddings)}")
        print("=" * 50)
        print("ğŸ”¢ å‘é‡åŒ–ç»“æŸ")
        print("=" * 50)
        
        logger.info(f"å¤šæ¨¡æ€æ•°æ®å‘é‡åŒ–å®Œæˆ:")
        logger.info(f"  - æ€»å›¾æ–‡å¯¹æ•°: {total_pairs}")
        logger.info(f"  - å›¾åƒå‘é‡æ•°: {image_vectors}")
        logger.info(f"  - æ–‡æœ¬å‘é‡æ•°: {text_vectors}")
        logger.info(f"  - æ€»æ–‡æ¡£æ•°: {len(documents)}")
        return documents, metadatas, embeddings
    
    def add_documents_to_db(self, vector_db, documents: List[str], metadatas: List[Dict], embeddings: List[np.ndarray] = None):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        print("=" * 50)
        print("ğŸ” è´¨é‡æ£€æŸ¥å¼€å§‹")
        print("=" * 50)
        logger.info("å¼€å§‹è´¨é‡æ£€æŸ¥å’Œæ–‡æ¡£æ·»åŠ ...")
        
        # è´¨é‡æ£€æŸ¥ç»Ÿè®¡
        total_docs = len(documents)
        valid_docs = 0
        invalid_docs = 0
        empty_docs = 0
        short_docs = 0
        
        logger.info(f"å¼€å§‹è´¨é‡æ£€æŸ¥ï¼Œæ€»æ–‡æ¡£æ•°: {total_docs}")
        
        # è´¨é‡æ£€æŸ¥ï¼šéªŒè¯æ–‡æ¡£å’Œå…ƒæ•°æ®
        for i, (doc, metadata) in enumerate(zip(documents, metadatas)):
            if not doc or len(doc.strip()) == 0:
                empty_docs += 1
                invalid_docs += 1
                logger.debug(f"æ–‡æ¡£ {i}: å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            if len(doc.strip()) < 5:
                short_docs += 1
                invalid_docs += 1
                logger.debug(f"æ–‡æ¡£ {i}: å†…å®¹è¿‡çŸ­ ({len(doc.strip())} å­—ç¬¦)ï¼Œè·³è¿‡")
                continue
                
            if not metadata:
                invalid_docs += 1
                logger.debug(f"æ–‡æ¡£ {i}: å…ƒæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            valid_docs += 1
        
        print("âœ… è´¨é‡æ£€æŸ¥å®Œæˆ")
        print(f"ğŸ“Š è´¨é‡æ£€æŸ¥ç»Ÿè®¡:")
        print(f"  - æ€»æ–‡æ¡£æ•°: {total_docs}")
        print(f"  - æœ‰æ•ˆæ–‡æ¡£æ•°: {valid_docs}")
        print(f"  - æ— æ•ˆæ–‡æ¡£æ•°: {invalid_docs}")
        print(f"  - ç©ºæ–‡æ¡£æ•°: {empty_docs}")
        print(f"  - çŸ­æ–‡æ¡£æ•°: {short_docs}")
        print("=" * 50)
        print("ğŸ” è´¨é‡æ£€æŸ¥ç»“æŸ")
        print("=" * 50)
        
        logger.info(f"è´¨é‡æ£€æŸ¥å®Œæˆ:")
        logger.info(f"  - æ€»æ–‡æ¡£æ•°: {total_docs}")
        logger.info(f"  - æœ‰æ•ˆæ–‡æ¡£æ•°: {valid_docs}")
        logger.info(f"  - æ— æ•ˆæ–‡æ¡£æ•°: {invalid_docs}")
        logger.info(f"  - ç©ºæ–‡æ¡£æ•°: {empty_docs}")
        logger.info(f"  - çŸ­æ–‡æ¡£æ•°: {short_docs}")
        
        print("=" * 50)
        print("ğŸ“š ç´¢å¼•æ„å»ºå¼€å§‹")
        print("=" * 50)
        logger.info(f"å¼€å§‹å°† {valid_docs} ä¸ªæœ‰æ•ˆæ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
        
        try:
            # æ‰¹é‡å¤„ç†
            for i in range(0, len(documents), self.config["BATCH_SIZE"]):
                batch_end = min(i + self.config["BATCH_SIZE"], len(documents))
                batch_docs = documents[i:batch_end]
                batch_metadatas = metadatas[i:batch_end]
                batch_ids = [f"doc_{i+j}" for j in range(len(batch_docs))]
                
                # å¦‚æœæä¾›äº†é¢„è®¡ç®—çš„åµŒå…¥å‘é‡ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
                if embeddings is not None and i < len(embeddings):
                    batch_embeddings = embeddings[i:batch_end]
                    vector_db.add_texts(
                        texts=batch_docs,
                        metadatas=batch_metadatas,
                        ids=batch_ids,
                        embeddings=batch_embeddings
                    )
                else:
                    vector_db.add_texts(
                        texts=batch_docs,
                        metadatas=batch_metadatas,
                        ids=batch_ids
                    )
                logger.info(f"å·²æ·»åŠ  {batch_end}/{len(documents)} ä¸ªæ–‡æ¡£")
            
            # æ•°æ®åº“ä¼šè‡ªåŠ¨æŒä¹…åŒ–
            print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
            print(f"ğŸ“Š ç´¢å¼•æ„å»ºç»Ÿè®¡:")
            print(f"  - æˆåŠŸæ·»åŠ æ–‡æ¡£æ•°: {valid_docs}")
            print(f"  - æ‰¹æ¬¡å¤§å°: {self.config['BATCH_SIZE']}")
            print(f"  - æ€»æ‰¹æ¬¡æ•°: {(len(documents) + self.config['BATCH_SIZE'] - 1) // self.config['BATCH_SIZE']}")
            print("=" * 50)
            print("ğŸ“š ç´¢å¼•æ„å»ºç»“æŸ")
            print("=" * 50)
            
            logger.info("æ–‡æ¡£æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            print("âŒ ç´¢å¼•æ„å»ºå¤±è´¥")
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            raise
    
    def add_image_vectors_to_db(self, vector_db, image_embeddings: np.ndarray, image_metadatas: List[Dict]):
        """æ‰¹é‡æ·»åŠ å›¾åƒå‘é‡åˆ°å‘é‡æ•°æ®åº“"""
        if image_embeddings is None or len(image_embeddings) == 0:
            logger.info("æ²¡æœ‰å›¾åƒå‘é‡å¯æ·»åŠ ")
            return
        
        logger.info(f"å°† {len(image_embeddings)} ä¸ªå›¾åƒå‘é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
        try:
            # æ‰¹é‡å¤„ç†
            for i in range(0, len(image_embeddings), self.config["IMAGE_BATCH_SIZE"]):
                batch_end = min(i + self.config["IMAGE_BATCH_SIZE"], len(image_embeddings))
                batch_embeddings = image_embeddings[i:batch_end]
                batch_metadatas = image_metadatas[i:batch_end]
                batch_ids = [f"image_{i+j}" for j in range(len(batch_embeddings))]
                
                # ä½¿ç”¨æ­£ç¡®çš„ChromaDB API
                vector_db._collection.add(
                    embeddings=batch_embeddings.tolist(),
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                logger.info(f"å·²æ·»åŠ  {batch_end}/{len(image_embeddings)} ä¸ªå›¾åƒå‘é‡")
            
            # æ•°æ®åº“ä¼šè‡ªåŠ¨æŒä¹…åŒ–
            logger.info("å›¾åƒå‘é‡æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ·»åŠ å›¾åƒå‘é‡åˆ°å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            raise
    
    def save_mapping(self, output_path: str = None):
        """ä¿å­˜å›¾åƒæ–‡æœ¬æ˜ å°„å…³ç³»"""
        if output_path is None:
            output_path = os.path.join(self.config["MULTIMODAL_VECTOR_DB_PATH"], "image_text_mapping.json")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.image_text_mapping, f, ensure_ascii=False, indent=2)
            logger.info(f"å›¾åƒæ–‡æœ¬æ˜ å°„å…³ç³»å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ˜ å°„å…³ç³»å¤±è´¥: {e}")
            raise
    
    def build_database(self, build_multimodal: bool = True):
        """
        æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        æ³¨æ„ï¼šå·²æ³¨é‡Šæ‰å•ç‹¬çš„æ–‡æœ¬å’Œå›¾åƒå‘é‡æ•°æ®åº“æ„å»ºï¼Œåªä¿ç•™å¤šæ¨¡æ€æ•°æ®åº“
        è¿™æ ·å¯ä»¥å‡å°‘å­˜å‚¨ç©ºé—´å’Œæ„å»ºæ—¶é—´ï¼Œæé«˜ç³»ç»Ÿæ•ˆç‡
        
        Args:
            build_multimodal: æ˜¯å¦æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“
        """
        try:
            print("=" * 80)
            print("ğŸš€ æ™ºè¯Šé€šå¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºæµç¨‹å¼€å§‹")
            print("=" * 80)
            print("ğŸ“‹ æ„å»ºæµç¨‹:")
            print("  1ï¸âƒ£ åŸå§‹æ•°æ® â†’ æ–‡æ¡£åŠ è½½")
            print("  2ï¸âƒ£ æ–‡æ¡£åŠ è½½ â†’ æ•°æ®æ¸…æ´—")
            print("  3ï¸âƒ£ æ•°æ®æ¸…æ´— â†’ æ–‡æ¡£åˆ‡åˆ†")
            print("  4ï¸âƒ£ æ–‡æ¡£åˆ‡åˆ† â†’ å‘é‡åŒ–")
            print("  5ï¸âƒ£ å‘é‡åŒ– â†’ è´¨é‡æ£€æŸ¥")
            print("  6ï¸âƒ£ è´¨é‡æ£€æŸ¥ â†’ ç´¢å¼•æ„å»º")
            print("=" * 80)
            
            logger.info("å¼€å§‹æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“...")
            
            # 1. åŠ è½½æ•°æ®
            data = self.load_data()
            
            # 2. å¤„ç†æ¯ç§æ•°æ®é›†
            datasets = [
                ("processed", data["processed_reports"], data["processed_images"]),
                ("train", data["train_reports"], data["train_images"]),
                ("test", data["test_reports"], data["test_images"])
            ]
            
            for dataset_type, reports_df, images in datasets:
                if reports_df.empty:
                    continue
                
                logger.info(f"\nå¤„ç† {dataset_type} æ•°æ®é›†...")
                
                # æ„å»ºæ–‡æœ¬å‘é‡æ•°æ®åº“ï¼ˆå·²æ³¨é‡Š - å†—ä½™åŠŸèƒ½ï¼‰
                # if build_text:
                #     logger.info(f"æ„å»º {dataset_type} æ–‡æœ¬å‘é‡æ•°æ®åº“...")
                #     docs, metadatas = self.prepare_text_documents(reports_df, images, dataset_type)
                #     self.add_documents_to_db(self.text_vector_db, docs, metadatas)
                
                # æ„å»ºå›¾åƒå‘é‡æ•°æ®åº“ï¼ˆå·²æ³¨é‡Š - å†—ä½™åŠŸèƒ½ï¼‰
                # if build_image and self.image_embedder and len(images) > 0:
                #     logger.info(f"æ„å»º {dataset_type} å›¾åƒå‘é‡æ•°æ®åº“...")
                #     # é™åˆ¶æ¯æ¬¡å¤„ç†çš„å›¾åƒæ•°é‡ï¼Œé˜²æ­¢å†…å­˜é—®é¢˜
                #     max_images_per_batch = self.config["MAX_IMAGES_PER_BATCH"]
                #     for i in range(0, len(images), max_images_per_batch):
                #         batch_end = min(i + max_images_per_batch, len(images))
                #         batch_images = images[i:batch_end]
                #         
                #         # å‘é‡åŒ–å½“å‰æ‰¹æ¬¡çš„å›¾åƒ
                #         image_embeddings, image_metadatas = self.vectorize_images(batch_images, dataset_type)
                #         
                #         # æ·»åŠ å›¾åƒå‘é‡åˆ°æ•°æ®åº“
                #         if image_embeddings is not None and self.image_vector_db is not None:
                #             self.add_image_vectors_to_db(self.image_vector_db, image_embeddings, image_metadatas)
                
                # æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“
                if build_multimodal:
                    logger.info(f"æ„å»º {dataset_type} å¤šæ¨¡æ€å‘é‡æ•°æ®åº“...")
                    documents, metadatas, embeddings = self.vectorize_multimodal_data(reports_df, images)
                    self.add_documents_to_db(self.multimodal_vector_db, documents, metadatas, embeddings)
            
            # 3. å¤„ç†çº¯æ–‡æœ¬æ•°æ®é›†
            general_text_datasets = [
                ("general_text_train", data["general_text_train"]),
                ("general_text_test", data["general_text_test"])
            ]
            
            for dataset_type, text_df in general_text_datasets:
                if text_df.empty:
                    continue
                
                logger.info(f"\nå¤„ç† {dataset_type} çº¯æ–‡æœ¬æ•°æ®é›†...")
                
                # æ„å»ºçº¯æ–‡æœ¬å‘é‡æ•°æ®åº“
                if build_multimodal:
                    logger.info(f"æ„å»º {dataset_type} çº¯æ–‡æœ¬å‘é‡æ•°æ®åº“...")
                    docs, metadatas = self.prepare_general_text_documents(text_df, dataset_type)
                    
                    # æ·»åŠ æ–‡æ¡£åˆ°å¤šæ¨¡æ€å‘é‡æ•°æ®åº“
                    if docs and self.multimodal_vector_db is not None:
                        self.add_documents_to_db(self.multimodal_vector_db, docs, metadatas)
            
            # ä¿å­˜æ˜ å°„å…³ç³»
            if build_multimodal:
                self.save_mapping()
            
            print("=" * 80)
            print("ğŸ‰ æ™ºè¯Šé€šå¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºæµç¨‹å®Œæˆï¼")
            print("=" * 80)
            print("ğŸ“Š æ„å»ºç»“æœæ€»ç»“:")
            print("  âœ… æ–‡æ¡£åŠ è½½: å®Œæˆ")
            print("  âœ… æ•°æ®æ¸…æ´—: å®Œæˆ")
            print("  âœ… æ–‡æ¡£åˆ‡åˆ†: å®Œæˆ")
            print("  âœ… å‘é‡åŒ–: å®Œæˆ")
            print("  âœ… è´¨é‡æ£€æŸ¥: å®Œæˆ")
            print("  âœ… ç´¢å¼•æ„å»º: å®Œæˆ")
            print("=" * 80)
            print("ğŸ“ æ•°æ®åº“ä½ç½®:")
            print(f"  - å¤šæ¨¡æ€å‘é‡æ•°æ®åº“: {self.config['MULTIMODAL_VECTOR_DB_PATH']}")
            print("=" * 80)
            print("ğŸš€ æ™ºè¯Šé€šå¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºæµç¨‹ç»“æŸ")
            print("=" * 80)
            
            logger.info("å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
            logger.info(f"å¤šæ¨¡æ€å‘é‡æ•°æ®åº“: {self.config['MULTIMODAL_VECTOR_DB_PATH']}")
            
        except Exception as e:
            print("=" * 80)
            print("âŒ æ™ºè¯Šé€šå¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºæµç¨‹å¤±è´¥ï¼")
            print("=" * 80)
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            print("=" * 80)
            logger.error(f"æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise




def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ„å»ºå™¨
        builder = UnifiedMultimodalVectorDatabaseBuilder()
        
        # æ„å»ºæ•°æ®åº“ï¼ˆåªæ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“ï¼‰
        builder.build_database(
            build_multimodal=True # æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“
        )
        
        # æ‰“å°é…ç½®æ‘˜è¦
        logger.info("\né…ç½®æ‘˜è¦:")
        logger.info(f"- æ–‡æœ¬åµŒå…¥æ¨¡å‹: {builder.config['TEXT_EMBEDDING_MODEL']}")
        logger.info(f"- å›¾åƒå‘é‡åŒ–: {'å¯ç”¨' if builder.config.get('IMAGE_EMBEDDING_ENABLED') and IMAGE_EMBEDDING_AVAILABLE else 'ç¦ç”¨'}")
        if builder.config.get('IMAGE_EMBEDDING_ENABLED') and IMAGE_EMBEDDING_AVAILABLE:
            logger.info(f"  - å‘é‡åŒ–å™¨ç±»å‹: {builder.config['IMAGE_EMBEDDER_TYPE']}")
            logger.info(f"  - æ¨¡å‹åç§°: {builder.config['IMAGE_EMBEDDING_MODEL']}")
            logger.info(f"  - ä½¿ç”¨è®¾å¤‡: {builder.config['IMAGE_EMBEDDER_DEVICE']}")
        
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()