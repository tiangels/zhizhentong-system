#!/usr/bin/env python3
"""
æ™ºè¯Šé€šç³»ç»Ÿæ¨¡å‹ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰AIæ¨¡å‹çš„ä¸‹è½½ã€é…ç½®å’Œä½¿ç”¨
"""

import os
import sys
import json
import logging
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import requests
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ç±»"""
    
    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            base_dir: æ¨¡å‹å­˜å‚¨åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        """
        if base_dir is None:
            # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
            self.base_dir = Path(__file__).parent
        else:
            self.base_dir = Path(base_dir)
        
        # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.models_config = {
            "llm_models": {
                "Qwen2-0.5B-Medical-MLX": {
                    "name": "Qwen2-0.5B-Medical-MLX",
                    "type": "llm",
                    "description": "åŒ»ç–—ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºQwen2-0.5Bå¾®è°ƒ",
                    "size": "34GB",
                    "files": [
                        "config.json",
                        "tokenizer_config.json", 
                        "tokenizer.json",
                        "model.safetensors.index.json",
                        "model-00001-of-00002.safetensors",
                        "model-00002-of-00002.safetensors"
                    ],
                    "download_url": None,  # æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½
                    "local_path": "Qwen2-0.5B-Medical-MLX"
                }
            },
            "embedding_models": {
                "text2vec-base-chinese": {
                    "name": "text2vec-base-chinese",
                    "type": "embedding",
                    "description": "ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
                    "size": "400MB",
                    "files": [
                        "config.json",
                        "sentence_bert_config.json",
                        "tokenizer_config.json",
                        "vocab.txt",
                        "model.safetensors",
                        "modules.json"
                    ],
                    "download_url": "https://huggingface.co/shibing624/text2vec-base-chinese",
                    "local_path": "text2vec-base-chinese"
                }
            }
        }
        
        # é…ç½®æ–‡ä»¶è·¯å¾„æ˜ å°„
        self.config_files = {
            "rag_config": "codes/services/knowledge_retrieval_service/api/config/rag_config.json",
            "unified_config": "codes/ai_models/embedding_models/config/unified_config.json",
            "medical_config": "codes/ai_models/embedding_models/config/medical_knowledge_config.json",
            "vector_config": "codes/ai_models/embedding_models/config/vector_config.json"
        }
    
    def check_model_exists(self, model_name: str, model_type: str = None) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹ (llm_models, embedding_models)
            
        Returns:
            æ¨¡å‹æ˜¯å¦å­˜åœ¨
        """
        # æ¨¡å‹ç›´æ¥å­˜å‚¨åœ¨ base_dir ä¸‹ï¼Œä¸åœ¨å­ç›®å½•ä¸­
        model_path = self.base_dir / model_name
        
        # ç›´æ¥æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        return model_path.exists() and model_path.is_dir()
    
    def _validate_model_files(self, model_path: Path, model_name: str) -> bool:
        """
        éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ–‡ä»¶æ˜¯å¦å®Œæ•´
        """
        # æŸ¥æ‰¾æ¨¡å‹é…ç½®
        model_config = None
        for mtype in self.models_config:
            if model_name in self.models_config[mtype]:
                model_config = self.models_config[mtype][model_name]
                break
        
        if not model_config:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„é…ç½®")
            return False
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        required_files = model_config.get("files", [])
        missing_files = []
        
        for file_name in required_files:
            file_path = model_path / file_name
            if not file_path.exists():
                missing_files.append(file_name)
        
        if missing_files:
            logger.warning(f"æ¨¡å‹ {model_name} ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            return False
        
        logger.info(f"æ¨¡å‹ {model_name} æ–‡ä»¶å®Œæ•´")
        return True
    
    def get_model_path(self, model_name: str, model_type: str = None) -> Optional[Path]:
        """
        è·å–æ¨¡å‹è·¯å¾„
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹
            
        Returns:
            æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if self.check_model_exists(model_name, model_type):
            return self.base_dir / model_name
        return None
    
    def list_models(self) -> Dict[str, List[Dict]]:
        """
        åˆ—å‡ºæ‰€æœ‰æ¨¡å‹åŠå…¶çŠ¶æ€
        
        Returns:
            æ¨¡å‹åˆ—è¡¨å’ŒçŠ¶æ€
        """
        result = {}
        
        for model_type, models in self.models_config.items():
            result[model_type] = []
            for model_name, model_config in models.items():
                model_path = self.base_dir / model_type / model_name
                exists = self.check_model_exists(model_name, model_type)
                
                result[model_type].append({
                    "name": model_name,
                    "description": model_config["description"],
                    "size": model_config["size"],
                    "exists": exists,
                    "path": str(model_path) if exists else None,
                    "local_path": model_config["local_path"]
                })
        
        return result
    
    def update_config_files(self, use_local_models: bool = True) -> bool:
        """
        æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„
        
        Args:
            use_local_models: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent.parent
            
            for config_name, config_path in self.config_files.items():
                full_config_path = project_root / config_path
                
                if not full_config_path.exists():
                    logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {full_config_path}")
                    continue
                
                # è¯»å–é…ç½®æ–‡ä»¶
                with open(full_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # æ›´æ–°æ¨¡å‹è·¯å¾„
                updated = False
                
                if config_name == "rag_config":
                    # æ›´æ–°RAGé…ç½®
                    if "llm_service" in config:
                        if use_local_models:
                            llm_path = self.get_model_path("Qwen2-0.5B-Medical-MLX", "llm_models")
                            if llm_path:
                                config["llm_service"]["model_path"] = str(llm_path)
                                updated = True
                    
                    if "vector_service" in config:
                        if use_local_models:
                            text_path = self.get_model_path("text2vec-base-chinese", "embedding_models")
                            if text_path:
                                config["vector_service"]["text_model_path"] = str(text_path)
                                updated = True
                
                elif config_name == "unified_config":
                    # æ›´æ–°ç»Ÿä¸€é…ç½®
                    if "models" in config:
                        if "text_embedding" in config["models"]:
                            if use_local_models:
                                text_path = self.get_model_path("text2vec-base-chinese", "embedding_models")
                                if text_path:
                                    config["models"]["text_embedding"]["model_path"] = str(text_path)
                                    updated = True
                
                elif config_name == "medical_config":
                    # æ›´æ–°åŒ»ç–—é…ç½®
                    if "models" in config:
                        if "text_embedding" in config["models"]:
                            if use_local_models:
                                text_path = self.get_model_path("text2vec-base-chinese", "embedding_models")
                                if text_path:
                                    config["models"]["text_embedding"]["model_path"] = str(text_path)
                                    updated = True
                
                elif config_name == "vector_config":
                    # æ›´æ–°å‘é‡é…ç½®
                    if use_local_models:
                        text_path = self.get_model_path("text2vec-base-chinese", "embedding_models")
                        if text_path:
                            config["LOCAL_MODEL_PATH"] = str(text_path)
                            updated = True
                
                # ä¿å­˜æ›´æ–°çš„é…ç½®
                if updated:
                    with open(full_config_path, 'w', encoding='utf-8') as f:
                        json.dump(config, f, indent=2, ensure_ascii=False)
                    logger.info(f"å·²æ›´æ–°é…ç½®æ–‡ä»¶: {config_path}")
                else:
                    logger.info(f"é…ç½®æ–‡ä»¶æ— éœ€æ›´æ–°: {config_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def create_model_symlink(self, model_name: str, target_path: str) -> bool:
        """
        åˆ›å»ºæ¨¡å‹ç¬¦å·é“¾æ¥
        
        Args:
            model_name: æ¨¡å‹åç§°
            target_path: ç›®æ ‡è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ›å»º
        """
        try:
            model_path = None
            for model_type in self.models_config:
                if model_name in self.models_config[model_type]:
                    model_path = self.base_dir / model_type / model_name
                    break
            
            if not model_path or not model_path.exists():
                logger.error(f"æ¨¡å‹ä¸å­˜åœ¨: {model_name}")
                return False
            
            target_path = Path(target_path)
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¦‚æœç›®æ ‡è·¯å¾„å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if target_path.exists():
                if target_path.is_symlink():
                    target_path.unlink()
                else:
                    shutil.rmtree(target_path)
            
            # åˆ›å»ºç¬¦å·é“¾æ¥
            target_path.symlink_to(model_path)
            logger.info(f"å·²åˆ›å»ºç¬¦å·é“¾æ¥: {target_path} -> {model_path}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥: {e}")
            return False
    
    def generate_model_guide(self) -> str:
        """
        ç”Ÿæˆæ¨¡å‹ä½¿ç”¨æŒ‡å—
        
        Returns:
            æŒ‡å—å†…å®¹
        """
        models_info = self.list_models()
        
        guide = """# æ™ºè¯Šé€šç³»ç»Ÿæœ¬åœ°æ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“ æ¨¡å‹å­˜å‚¨ç»“æ„

```
codes/ai_models/llm_models/
â”œâ”€â”€ llm_models/                    # å¤§è¯­è¨€æ¨¡å‹ç›®å½•
â”‚   â””â”€â”€ Medical_Qwen3_17B/        # åŒ»ç–—ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹
â””â”€â”€ embedding_models/              # å‘é‡åŒ–æ¨¡å‹ç›®å½•
    â””â”€â”€ text2vec-base-chinese/    # ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
```

## ğŸ¤– æ¨¡å‹åˆ—è¡¨

"""
        
        for model_type, models in models_info.items():
            guide += f"### {model_type.replace('_', ' ').title()}\n\n"
            
            for model in models:
                status = "âœ… å·²å®‰è£…" if model["exists"] else "âŒ æœªå®‰è£…"
                guide += f"#### {model['name']}\n"
                guide += f"- **æè¿°**: {model['description']}\n"
                guide += f"- **å¤§å°**: {model['size']}\n"
                guide += f"- **çŠ¶æ€**: {status}\n"
                if model["path"]:
                    guide += f"- **è·¯å¾„**: `{model['path']}`\n"
                guide += "\n"
        
        guide += """## ğŸ”§ é…ç½®è¯´æ˜

### è‡ªåŠ¨é…ç½®æ›´æ–°

æ¨¡å‹ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ›´æ–°ä»¥ä¸‹é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„ï¼š

1. **RAGæœåŠ¡é…ç½®**: `codes/services/knowledge_retrieval_service/api/config/rag_config.json`
2. **ç»Ÿä¸€é…ç½®**: `codes/ai_models/embedding_models/config/unified_config.json`
3. **åŒ»ç–—é…ç½®**: `codes/ai_models/embedding_models/config/medical_knowledge_config.json`
4. **å‘é‡é…ç½®**: `codes/ai_models/embedding_models/config/vector_config.json`

### æ‰‹åŠ¨é…ç½®

å¦‚æœéœ€è¦æ‰‹åŠ¨é…ç½®æ¨¡å‹è·¯å¾„ï¼Œè¯·ç¡®ä¿è·¯å¾„æŒ‡å‘æ­£ç¡®çš„æ¨¡å‹ç›®å½•ã€‚

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€

```python
from model_manager import ModelManager

manager = ModelManager()
models = manager.list_models()
print(models)
```

### 2. æ›´æ–°é…ç½®æ–‡ä»¶

```python
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„æ›´æ–°æ‰€æœ‰é…ç½®æ–‡ä»¶
manager.update_config_files(use_local_models=True)
```

### 3. éªŒè¯æ¨¡å‹å®Œæ•´æ€§

```python
# æ£€æŸ¥ç‰¹å®šæ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
exists = manager.check_model_exists("Medical_Qwen3_17B", "llm_models")
print(f"æ¨¡å‹å­˜åœ¨: {exists}")
```

## ğŸ“‹ æ¨¡å‹è¯¦ç»†ä¿¡æ¯

### Medical_Qwen3_17B

- **ç±»å‹**: å¤§è¯­è¨€æ¨¡å‹ (LLM)
- **ç”¨é€”**: åŒ»ç–—é—®ç­”ã€è¯Šæ–­å»ºè®®ã€å¥åº·å’¨è¯¢
- **ç‰¹ç‚¹**: åŸºäºQwen3-17Bå¾®è°ƒï¼Œä¸“é—¨é’ˆå¯¹åŒ»ç–—åœºæ™¯ä¼˜åŒ–
- **å†…å­˜éœ€æ±‚**: å»ºè®®32GB+ RAM
- **GPUæ”¯æŒ**: æ”¯æŒCUDAåŠ é€Ÿ

### text2vec-base-chinese

- **ç±»å‹**: æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
- **ç”¨é€”**: æ–‡æœ¬åµŒå…¥ã€è¯­ä¹‰æœç´¢ã€æ–‡æ¡£æ£€ç´¢
- **ç‰¹ç‚¹**: ä¸­æ–‡ä¼˜åŒ–ï¼Œæ”¯æŒåŒ»ç–—æ–‡æœ¬å‘é‡åŒ–
- **å†…å­˜éœ€æ±‚**: å»ºè®®4GB+ RAM
- **GPUæ”¯æŒ**: å¯é€‰

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´**
   - æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
   - éªŒè¯å¿…éœ€æ–‡ä»¶æ˜¯å¦é½å…¨
   - é‡æ–°ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹æ–‡ä»¶

2. **é…ç½®æ–‡ä»¶è·¯å¾„é”™è¯¯**
   - è¿è¡Œ `manager.update_config_files()` è‡ªåŠ¨æ›´æ–°
   - æ‰‹åŠ¨æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„è®¾ç½®

3. **å†…å­˜ä¸è¶³**
   - å‡å°‘æ‰¹å¤„ç†å¤§å°
   - ä½¿ç”¨CPUæ¨ç†
   - å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹æ¨¡å‹ç®¡ç†å™¨æ—¥å¿—
tail -f logs/model_manager.log

# æŸ¥çœ‹RAGæœåŠ¡æ—¥å¿—
tail -f codes/services/knowledge_retrieval_service/logs/rag_service.log
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
2. é…ç½®æ–‡ä»¶è·¯å¾„
3. ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
4. ä¾èµ–åŒ…ç‰ˆæœ¬

---

**æ›´æ–°æ—¶é—´**: 2024å¹´9æœˆ11æ—¥  
**ç‰ˆæœ¬**: 1.0.0  
**çŠ¶æ€**: âœ… å°±ç»ª
"""
        
        return guide
    
    def save_model_guide(self, output_path: str = None) -> bool:
        """
        ä¿å­˜æ¨¡å‹ä½¿ç”¨æŒ‡å—åˆ°æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸä¿å­˜
        """
        try:
            if output_path is None:
                output_path = self.base_dir / "æœ¬åœ°æ¨¡å‹ä½¿ç”¨æŒ‡å—.md"
            
            guide_content = self.generate_model_guide()
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(guide_content)
            
            logger.info(f"æ¨¡å‹ä½¿ç”¨æŒ‡å—å·²ä¿å­˜åˆ°: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹ä½¿ç”¨æŒ‡å—å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ™ºè¯Šé€šç³»ç»Ÿæ¨¡å‹ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    manager = ModelManager()
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
    print("\nğŸ“‹ æ¨¡å‹çŠ¶æ€:")
    models = manager.list_models()
    
    for model_type, model_list in models.items():
        print(f"\n{model_type.replace('_', ' ').title()}:")
        for model in model_list:
            status = "âœ…" if model["exists"] else "âŒ"
            print(f"  {status} {model['name']} - {model['description']}")
    
    # æ›´æ–°é…ç½®æ–‡ä»¶
    print("\nğŸ”§ æ›´æ–°é…ç½®æ–‡ä»¶...")
    if manager.update_config_files(use_local_models=True):
        print("âœ… é…ç½®æ–‡ä»¶æ›´æ–°æˆåŠŸ")
    else:
        print("âŒ é…ç½®æ–‡ä»¶æ›´æ–°å¤±è´¥")
    
    # ç”Ÿæˆå¹¶ä¿å­˜ä½¿ç”¨æŒ‡å—
    print("\nğŸ“– ç”Ÿæˆæ¨¡å‹ä½¿ç”¨æŒ‡å—...")
    if manager.save_model_guide():
        print("âœ… æ¨¡å‹ä½¿ç”¨æŒ‡å—ç”ŸæˆæˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹ä½¿ç”¨æŒ‡å—ç”Ÿæˆå¤±è´¥")
    
    print("\nğŸ‰ æ¨¡å‹ç®¡ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
