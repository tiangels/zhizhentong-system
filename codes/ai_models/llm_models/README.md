# æ™ºè¯Šé€šç³»ç»Ÿ - æœ¬åœ°æ¨¡å‹ç®¡ç†

## ğŸ“‹ æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«æ™ºè¯Šé€šç³»ç»Ÿä½¿ç”¨çš„æ‰€æœ‰æœ¬åœ°AIæ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡åŒ–æ¨¡å‹ã€‚ç³»ç»Ÿå·²å®Œå…¨é…ç½®ä¸ºä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œç¡®ä¿ç¦»çº¿è¿è¡Œå’Œå¿«é€Ÿå¯åŠ¨ã€‚

## ğŸ—‚ï¸ ç›®å½•ç»“æ„

```
codes/ai_models/llm_models/
â”œâ”€â”€ Medical_Qwen3_17B/            # åŒ»ç–—ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ (34GB)
â”œâ”€â”€ text2vec-base-chinese/        # ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹ (400MB)
â”œâ”€â”€ model_manager.py              # æ¨¡å‹ç®¡ç†å™¨ â­
â”œâ”€â”€ check_project_models.py       # é¡¹ç›®æ¨¡å‹ä½¿ç”¨æ£€æŸ¥å™¨ â­
â”œâ”€â”€ README.md                     # æœ¬è¯´æ˜æ–‡æ¡£
â””â”€â”€ __pycache__/                  # Pythonç¼“å­˜ç›®å½•
```

## ğŸ¤– æ¨¡å‹ä¿¡æ¯

### Medical_Qwen3_17B - åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹

- **æ¨¡å‹ç±»å‹**: åŒ»ç–—é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹
- **å‚æ•°é‡**: 17B
- **æ¨¡å‹å¤§å°**: 34GB
- **ç”¨é€”**: åŒ»ç–—æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€è¯Šæ–­å»ºè®®
- **æœ¬åœ°è·¯å¾„**: `Medical_Qwen3_17B/`

#### æ¨¡å‹æ–‡ä»¶ç»“æ„
```
Medical_Qwen3_17B/
â”œâ”€â”€ config.json                    # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ generation_config.json         # ç”Ÿæˆé…ç½®
â”œâ”€â”€ tokenizer_config.json          # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer.json                 # åˆ†è¯å™¨æ–‡ä»¶
â”œâ”€â”€ vocab.json                     # è¯æ±‡è¡¨
â”œâ”€â”€ merges.txt                     # BPEåˆå¹¶è§„åˆ™
â”œâ”€â”€ added_tokens.json              # é¢å¤–token
â”œâ”€â”€ special_tokens_map.json        # ç‰¹æ®Štokenæ˜ å°„
â”œâ”€â”€ model.safetensors.index.json   # æ¨¡å‹ç´¢å¼•æ–‡ä»¶
â”œâ”€â”€ model-00001-of-00002.safetensors  # æ¨¡å‹æƒé‡æ–‡ä»¶1 (1.98GB)
â”œâ”€â”€ model-00002-of-00002.safetensors  # æ¨¡å‹æƒé‡æ–‡ä»¶2 (1.46GB)
â”œâ”€â”€ README.md                      # æ¨¡å‹è¯´æ˜æ–‡æ¡£
â””â”€â”€ Modelfile                      # æ¨¡å‹æ–‡ä»¶æè¿°
```

### text2vec-base-chinese - ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹

- **æ¨¡å‹ç±»å‹**: ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
- **å‘é‡ç»´åº¦**: 768
- **æ¨¡å‹å¤§å°**: 400MB
- **ç”¨é€”**: æ–‡æœ¬å‘é‡åŒ–ã€è¯­ä¹‰æœç´¢ã€ç›¸ä¼¼åº¦è®¡ç®—
- **æœ¬åœ°è·¯å¾„**: `text2vec-base-chinese/`

## ğŸ› ï¸ ç®¡ç†å·¥å…·

### 1. æ¨¡å‹ç®¡ç†å™¨ (model_manager.py)

ç»Ÿä¸€ç®¡ç†æ‰€æœ‰AIæ¨¡å‹çš„é…ç½®å’Œä½¿ç”¨ï¼š

**åŠŸèƒ½**:
- æ£€æŸ¥æ¨¡å‹çŠ¶æ€å’Œå®Œæ•´æ€§
- è‡ªåŠ¨æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„
- ç”Ÿæˆè¯¦ç»†çš„æ¨¡å‹ä½¿ç”¨æŒ‡å—
- éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

**ä½¿ç”¨æ–¹æ³•**:
```bash
cd codes/ai_models/llm_models
python model_manager.py
```

**ä¸»è¦åŠŸèƒ½**:
```python
from model_manager import ModelManager

manager = ModelManager()

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
models = manager.list_models()
print(models)

# æ›´æ–°é…ç½®æ–‡ä»¶
manager.update_config_files(use_local_models=True)

# éªŒè¯æ¨¡å‹å®Œæ•´æ€§
exists = manager.check_model_exists("Medical_Qwen3_17B")
print(f"æ¨¡å‹å­˜åœ¨: {exists}")
```

### 2. é¡¹ç›®æ¨¡å‹ä½¿ç”¨æ£€æŸ¥å™¨ (check_project_models.py)

æ£€æŸ¥é¡¹ç›®ä¸­æ‰€æœ‰æ¨¡å‹ä½¿ç”¨çš„åœ°æ–¹ï¼š

**åŠŸèƒ½**:
- æ‰«æé¡¹ç›®æ–‡ä»¶ï¼Œæ£€æŸ¥æ¨¡å‹ä½¿ç”¨æƒ…å†µ
- æ’é™¤ç¬¬ä¸‰æ–¹åº“ï¼Œåªæ£€æŸ¥é¡¹ç›®æ–‡ä»¶
- ç”Ÿæˆè¯¦ç»†çš„æ£€æŸ¥æŠ¥å‘Š
- è¯†åˆ«éœ€è¦æ›´æ–°çš„æ–‡ä»¶

**ä½¿ç”¨æ–¹æ³•**:
```bash
python check_project_models.py
```

**æ£€æŸ¥æŠ¥å‘Š**:
- `é¡¹ç›®æ¨¡å‹ä½¿ç”¨æ£€æŸ¥æŠ¥å‘Š.md` - è¯¦ç»†çš„é¡¹ç›®æ¨¡å‹ä½¿ç”¨æƒ…å†µ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€

```bash
cd codes/ai_models/llm_models
python model_manager.py
```

### 2. éªŒè¯ç³»ç»Ÿé…ç½®

```bash
python check_project_models.py
```

### 3. æµ‹è¯•æ¨¡å‹åŠŸèƒ½

```python
from codes.backend.app.services.rag_service import get_rag_service

rag_service = get_rag_service()
print(f'RAGæœåŠ¡å¯ç”¨: {rag_service.is_available()}')

if rag_service.is_available():
    print('âœ… ç³»ç»Ÿé…ç½®æ­£ç¡®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹')
else:
    print('âŒ ç³»ç»Ÿé…ç½®æœ‰é—®é¢˜')
```

## ğŸ“Š ç³»ç»ŸçŠ¶æ€

### âœ… å½“å‰çŠ¶æ€
- **Medical_Qwen3_17B**: âœ… å·²å®‰è£…å¹¶å¯ç”¨
- **text2vec-base-chinese**: âœ… å·²å®‰è£…å¹¶å¯ç”¨
- **RAGæœåŠ¡**: âœ… æ­£å¸¸ä½¿ç”¨æœ¬åœ°æ¨¡å‹
- **å‘é‡åŒ–æœåŠ¡**: âœ… æ­£å¸¸ä½¿ç”¨æœ¬åœ°æ¨¡å‹
- **LLMæœåŠ¡**: âœ… æ­£å¸¸ä½¿ç”¨æœ¬åœ°æ¨¡å‹

### âœ… é…ç½®çŠ¶æ€
- **æ‰€æœ‰é…ç½®æ–‡ä»¶**: âœ… å·²æ›´æ–°ä¸ºæœ¬åœ°è·¯å¾„
- **æ¨¡å‹è·¯å¾„**: âœ… ç»Ÿä¸€ç®¡ç†
- **è‡ªåŠ¨æ£€æŸ¥**: âœ… ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
- **æœ¬åœ°åŒ–ç‡**: âœ… 100%

## ğŸ”§ é…ç½®ç®¡ç†

### è‡ªåŠ¨é…ç½®æ›´æ–°

ç³»ç»Ÿä¼šè‡ªåŠ¨æ›´æ–°ä»¥ä¸‹é…ç½®æ–‡ä»¶ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š

1. **RAGæœåŠ¡é…ç½®**: `codes/services/knowledge_retrieval_service/api/config/rag_config.json`
2. **ç»Ÿä¸€é…ç½®**: `codes/ai_models/embedding_models/config/unified_config.json`
3. **åŒ»ç–—é…ç½®**: `codes/ai_models/embedding_models/config/medical_knowledge_config.json`
4. **å‘é‡é…ç½®**: `codes/ai_models/embedding_models/config/vector_config.json`

### æ‰‹åŠ¨é…ç½®

å¦‚æœéœ€è¦æ‰‹åŠ¨é…ç½®æ¨¡å‹è·¯å¾„ï¼Œè¯·ç¡®ä¿è·¯å¾„æŒ‡å‘æ­£ç¡®çš„æ¨¡å‹ç›®å½•ã€‚

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### 1. ä½¿ç”¨ Transformers åº“åŠ è½½

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "codes/ai_models/llm_models/Medical_Qwen3_17B"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜
    device_map="auto"           # è‡ªåŠ¨åˆ†é…è®¾å¤‡
)

# ç”Ÿæˆæ–‡æœ¬
def generate_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 2. ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†ï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```python
from vllm import LLM, SamplingParams

model_path = "codes/ai_models/llm_models/Medical_Qwen3_17B"

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model=model_path,
    tensor_parallel_size=1,  # æ ¹æ®GPUæ•°é‡è°ƒæ•´
    dtype="half"             # ä½¿ç”¨åŠç²¾åº¦
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)

# ç”Ÿæˆå“åº”
def generate_medical_response(prompt):
    outputs = llm.generate([prompt], sampling_params)
    return outputs[0].outputs[0].text
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹è·¯å¾„é”™è¯¯**
   ```bash
   # è¿è¡Œæ¨¡å‹ç®¡ç†å™¨è‡ªåŠ¨ä¿®å¤
   python model_manager.py
   ```

2. **é…ç½®æ–‡ä»¶ä¸ä¸€è‡´**
   ```bash
   # æ£€æŸ¥é¡¹ç›®æ¨¡å‹ä½¿ç”¨æƒ…å†µ
   python check_project_models.py
   ```

3. **æ¨¡å‹æ–‡ä»¶ç¼ºå¤±**
   - æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
   - éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
   - é‡æ–°ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹

### æ£€æŸ¥æ¸…å•

- [ ] æ‰€æœ‰Pythonæ–‡ä»¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
- [ ] é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„æ­£ç¡®
- [ ] å¯åŠ¨è„šæœ¬æ£€æŸ¥æœ¬åœ°æ¨¡å‹å­˜åœ¨æ€§
- [ ] æ–‡æ¡£ä¸­çš„æ¨¡å‹è·¯å¾„ä¿¡æ¯å‡†ç¡®

## ğŸ“‹ ç¡¬ä»¶è¦æ±‚

### Medical_Qwen3_17B
- **æœ€ä½è¦æ±‚**: 16GB RAM, 8GB VRAM
- **æ¨èé…ç½®**: 32GB RAM, 16GB+ VRAM
- **GPU**: æ”¯æŒCUDAçš„NVIDIA GPUï¼ˆæ¨èRTX 3080æˆ–æ›´é«˜ï¼‰

### text2vec-base-chinese
- **æœ€ä½è¦æ±‚**: 4GB RAM
- **æ¨èé…ç½®**: 8GB RAM
- **GPU**: å¯é€‰ï¼ŒCPUä¹Ÿå¯æ­£å¸¸è¿è¡Œ

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
2. é…ç½®æ–‡ä»¶è·¯å¾„
3. ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
4. ä¾èµ–åŒ…ç‰ˆæœ¬

### å®šæœŸç»´æŠ¤

```bash
# æ¯æœˆè¿è¡Œä¸€æ¬¡æ¨¡å‹æ£€æŸ¥
python model_manager.py
python check_project_models.py
```

### æ·»åŠ æ–°æ¨¡å‹

1. å°†æ¨¡å‹æ–‡ä»¶æ”¾å…¥ `codes/ai_models/llm_models/` ç›®å½•
2. è¿è¡Œ `python model_manager.py` æ›´æ–°é…ç½®
3. è¿è¡Œ `python check_project_models.py` éªŒè¯

---

**æ›´æ–°æ—¶é—´**: 2024å¹´9æœˆ11æ—¥  
**ç‰ˆæœ¬**: 2.0.0  
**çŠ¶æ€**: âœ… å°±ç»ª  
**ç»´æŠ¤è€…**: æ™ºè¯Šé€šç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
